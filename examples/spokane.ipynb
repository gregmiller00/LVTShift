{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For this analysis, we're modeling a **4:1 split-rate tax** where land is taxed at four times the rate of buildings.\n",
    "\n",
    "## Policy Definition for Spokane\n",
    "\n",
    "\n",
    "For this analysis, we're modeling:\n",
    "- **Revenue-neutral** property tax split for the South Bend School Corporation\n",
    "- **4:1 land-to-building tax ratio** (partial LVT shift)\n",
    "- **Existing exemptions and abatements** continue to apply\n",
    "- **Focus on school corporation taxes** (which bypass Indiana's property tax caps)\n",
    "\n",
    "Let's begin by importing the necessary libraries and utility functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility functions\n",
    "import sys\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "# Import from local utility modules\n",
    "from lvt_utils import ensure_geodataframe\n",
    "\n",
    "print(\"✅ Utility functions imported from lvt_utils\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules and functions\n",
    "from cloud_utils import get_feature_data, get_feature_data_with_geometry\n",
    "from lvt_utils import (model_split_rate_tax, calculate_current_tax, model_full_building_abatement, \n",
    "                       model_stacking_improvement_exemption, categorize_property_type, extract_date_from_filename)\n",
    "from census_utils import (get_census_data, get_census_blockgroups_shapefile, get_census_data_with_boundaries, \n",
    "                          match_to_census_blockgroups, calculate_median_percentage_by_quintile, \n",
    "                          match_parcels_to_demographics, create_demographic_summary)\n",
    "from viz import (create_scatter_plot, plot_comparison, calculate_correlations, weighted_median, \n",
    "                 create_quintile_summary, plot_quintile_analysis, create_property_category_chart, \n",
    "                 create_map_visualization, calculate_block_group_summary, filter_data_for_analysis)\n",
    "\n",
    "scrape_data = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Getting the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gismo.spokanecounty.org/arcgis/rest/services/SCOUT/PropertyLookup/MapServer/0/query?f=json&where=PID_NUM%20%3D%20%2725011.1709%27&returnGeometry=false&spatialRel=esriSpatialRelIntersects&outFields=site_address%2Csite_city%2Cowner_name%2Cdocument_date%2Cgross_sale_price%2Cexcise_nbr%2Ctransfer_type%2Cprop_use_desc%2Ctax_code_area%2Cacreage%2CInspectionYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# Directory to save/load data\n",
    "data_dir = \"data/spokane\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "if scrape_data == 1:\n",
    "    # Base URL for the ArcGIS services\n",
    "    base_url = \"https://services1.arcgis.com/ozNll27nt9ZtPWOn/ArcGIS/rest/services/\"\n",
    "    # Fetch the main parcel dataset with tax info\n",
    "    parcel_civic_df = get_feature_data_with_geometry('Parcels', base_url)\n",
    "    # Save with geometry to parquet, with current date\n",
    "    today_str = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "    out_path = os.path.join(data_dir, f\"spokane_parcels_{today_str}.parquet\")\n",
    "    parcel_civic_df.to_parquet(out_path, index=False)\n",
    "    print(f\"Saved new scrape to {out_path}\")\n",
    "else:\n",
    "    # Find the most recent parquet file in the data_dir\n",
    "    files = glob.glob(os.path.join(data_dir, \"spokane_parcels_*.parquet\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No previously scraped parcel files found in data/spokane/\")\n",
    "    # Sort files by date in filename\n",
    "    files_sorted = sorted(files, key=lambda x: datetime.strptime(os.path.basename(x).replace(\"spokane_parcels_\", \"\").replace(\".parquet\", \"\"), \"%Y_%m_%d\"), reverse=True)\n",
    "    latest_file = files_sorted[0]\n",
    "    print(f\"Loading most recent scrape: {latest_file}\")\n",
    "    parcel_civic_df = pd.read_parquet(latest_file)\n",
    "\n",
    "# Ensure parcel_civic_df is a proper GeoDataFrame\n",
    "parcel_civic_df = ensure_geodataframe(parcel_civic_df)\n",
    "print(f\"✅ Parcel data loaded as {type(parcel_civic_df).__name__} with CRS: {parcel_civic_df.crs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charge data comes from:  https://gisdatacatalog-spokanecounty.opendata.arcgis.com/pages/parcel-data-file-downloads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(parcel_civic_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load charge_info_1 and charge_info_2 from Excel files in data/spokane/\n",
    "charge_info_1 = pd.read_excel(os.path.join(data_dir, \"charge_info_1.xlsx\"))\n",
    "charge_info_2 = pd.read_excel(os.path.join(data_dir, \"charge_info_2.xlsx\"))\n",
    "\n",
    "# Check if charge_info_1 and charge_info_2 have the same columns\n",
    "if list(charge_info_1.columns) == list(charge_info_2.columns):\n",
    "    millage_df = pd.concat([charge_info_1, charge_info_2], ignore_index=True)\n",
    "else:\n",
    "    print(\"❌ charge_info_1 and charge_info_2 do not have the same column names.\")\n",
    "    millage_df = None\n",
    "\n",
    "# Display the dataframe if it was created\n",
    "if millage_df is not None:\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    display(millage_df.head(5))\n",
    "\n",
    "\n",
    "print(\"Counts of charge_type in millage_df:\")\n",
    "if millage_df is not None:\n",
    "    print(millage_df['charge_type'].value_counts(dropna=False).to_string())\n",
    "else:\n",
    "    print(\"millage_df is None, cannot print charge_type counts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Column names in parcel_civic_df:\")\n",
    "print(parcel_civic_df.columns.tolist())\n",
    "pd.set_option('display.max_columns', None)\n",
    "display(parcel_civic_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parcel_civic_df['full_exmp'] = (parcel_civic_df['taxable_amt'] <= 0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Counts of exmp_code in parcel_civic_df:\")\n",
    "print(parcel_civic_df['exmp_code'].value_counts(dropna=False).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def load_levy_table(pid):\n",
    "    \"\"\"\n",
    "    Loads the levy table for a given PID from the Spokane County property information site.\n",
    "    Returns a DataFrame with columns: levy_name, levy_rate_24, levy_rate_25, levy_type\n",
    "    Includes the total row.\n",
    "    \"\"\"\n",
    "    url = f\"https://cp.spokanecounty.org/SCOUT/propertyinformation/Summary.aspx?PID={pid}\"\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    table = soup.find(\"table\", id=\"MainContent_Levy_GridView1\")\n",
    "    if table is None:\n",
    "        raise ValueError(\"Could not find levy table in the page.\")\n",
    "\n",
    "    rows = table.find_all(\"tr\")\n",
    "    data = []\n",
    "    for tr in rows[1:]:  # skip header row\n",
    "        tds = tr.find_all(\"td\")\n",
    "        if len(tds) < 4:\n",
    "            continue\n",
    "        # Get text, strip whitespace\n",
    "        levy_name = tds[0].get_text(strip=True)\n",
    "        levy_rate_24 = tds[1].get_text(strip=True)\n",
    "        levy_rate_25 = tds[2].get_text(strip=True)\n",
    "        levy_type = tds[3].get_text(strip=True)\n",
    "        data.append({\n",
    "            \"levy_name\": levy_name,\n",
    "            \"levy_rate_24\": levy_rate_24,\n",
    "            \"levy_rate_25\": levy_rate_25,\n",
    "            \"levy_type\": levy_type\n",
    "        })\n",
    "    # Only keep rows with at least one non-empty rate or the Totals row\n",
    "    filtered = []\n",
    "    for row in data:\n",
    "        if row[\"levy_name\"].lower().startswith(\"totals\"):\n",
    "            filtered.append(row)\n",
    "        elif row[\"levy_rate_24\"] or row[\"levy_rate_25\"]:\n",
    "            filtered.append(row)\n",
    "    df = pd.DataFrame(filtered)\n",
    "    return df\n",
    "\n",
    "# Test the function with the given PID\n",
    "test_pid = \"35182.4601\"\n",
    "levy_df = load_levy_table(test_pid)\n",
    "print(levy_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "millage_dir = \"data/spokane/\"\n",
    "os.makedirs(millage_dir, exist_ok=True)\n",
    "\n",
    "def extract_date_from_filename(path):\n",
    "    \"\"\"\n",
    "    Extracts a datetime object from a filename like millage_scrape_YYYY_MM_DD.parquet\n",
    "    Returns None if parsing fails.\n",
    "    \"\"\"\n",
    "    base = os.path.basename(path)\n",
    "    # Expecting: millage_scrape_YYYY_MM_DD.parquet\n",
    "    parts = base.replace(\".parquet\", \"\").split(\"_\")\n",
    "    if len(parts) >= 4:\n",
    "        try:\n",
    "            # The last three parts should be YYYY, MM, DD\n",
    "            date_str = \"_\".join(parts[-3:])\n",
    "            return datetime.strptime(date_str, \"%Y_%m_%d\")\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "if scrape_data == 1:\n",
    "    # Get the first PID for each unique tax_code_area\n",
    "    tax_code_area_pid = (\n",
    "        parcel_civic_df.dropna(subset=['tax_code_area', 'PID_NUM'])\n",
    "        .sort_values('PID_NUM')\n",
    "        .groupby('tax_code_area')['PID_NUM']\n",
    "        .first()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    levy_records = []\n",
    "    for _, row in tqdm(tax_code_area_pid.iterrows(), total=len(tax_code_area_pid), desc=\"Pulling levy tables\"):\n",
    "        tax_code_area = row['tax_code_area']\n",
    "        pid = row['PID_NUM']\n",
    "        # Try up to two times with this PID, then try another PID from the same tax_code_area if both fail\n",
    "        success = False\n",
    "        attempts = 0\n",
    "        tried_pids = set()\n",
    "        tried_pids.add(pid)\n",
    "        while not success and attempts < 2:\n",
    "            try:\n",
    "                levy_df = load_levy_table(pid)\n",
    "                levy_df['tax_code_area'] = tax_code_area\n",
    "                levy_records.append(levy_df)\n",
    "                success = True\n",
    "            except Exception as e:\n",
    "                attempts += 1\n",
    "                print(f\"Attempt {attempts} failed to load levy table for tax_code_area {tax_code_area} (PID {pid}): {e}\")\n",
    "                if attempts < 2:\n",
    "                    continue  # Try again with the same PID\n",
    "                # If failed twice, try another PID from the same tax_code_area\n",
    "                other_pids = (\n",
    "                    parcel_civic_df[\n",
    "                        (parcel_civic_df['tax_code_area'] == tax_code_area) &\n",
    "                        (parcel_civic_df['PID_NUM'] != pid)\n",
    "                    ]['PID_NUM']\n",
    "                    .dropna()\n",
    "                    .unique()\n",
    "                )\n",
    "                found_alternate = False\n",
    "                for alt_pid in other_pids:\n",
    "                    if alt_pid in tried_pids:\n",
    "                        continue\n",
    "                    try:\n",
    "                        levy_df = load_levy_table(alt_pid)\n",
    "                        levy_df['tax_code_area'] = tax_code_area\n",
    "                        levy_records.append(levy_df)\n",
    "                        print(f\"Success with alternate PID {alt_pid} for tax_code_area {tax_code_area}\")\n",
    "                        found_alternate = True\n",
    "                        break\n",
    "                    except Exception as e2:\n",
    "                        print(f\"Alternate PID {alt_pid} also failed for tax_code_area {tax_code_area}: {e2}\")\n",
    "                        tried_pids.add(alt_pid)\n",
    "                if not found_alternate:\n",
    "                    print(f\"All attempts failed for tax_code_area {tax_code_area}\")\n",
    "\n",
    "    # Concatenate all levy records into a single DataFrame\n",
    "    all_levies_df = pd.concat(levy_records, ignore_index=True)\n",
    "\n",
    "    # Reorder columns as requested\n",
    "    all_levies_df = all_levies_df[['tax_code_area', 'levy_name', 'levy_rate_24', 'levy_rate_25', 'levy_type']]\n",
    "\n",
    "    # Save to file with current date\n",
    "    today_str = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "    millage_path = os.path.join(millage_dir, f\"millage_scrape_{today_str}.parquet\")\n",
    "    all_levies_df.to_parquet(millage_path, index=False)\n",
    "    print(f\"Saved millage rates to {millage_path}\")\n",
    "\n",
    "else:\n",
    "    # Find the most recent millage_scrape_*.parquet file\n",
    "    files = glob.glob(os.path.join(millage_dir, \"millage_scrape_*.parquet\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No millage_scrape_*.parquet files found in data/spokane/. Set data_scrape=1 to scrape.\")\n",
    "    # Sort by date in filename robustly\n",
    "    dated_files = []\n",
    "    for f in files:\n",
    "        dt = extract_date_from_filename(f)\n",
    "        if dt is not None:\n",
    "            dated_files.append((dt, f))\n",
    "    if dated_files:\n",
    "        # Pick the file with the latest date\n",
    "        most_recent = max(dated_files, key=lambda tup: tup[0])[1]\n",
    "    else:\n",
    "        # Fallback: sort by mtime\n",
    "        most_recent = max(files, key=os.path.getmtime)\n",
    "    all_levies_df = pd.read_parquet(most_recent)\n",
    "    print(f\"Loaded millage rates from {most_recent}\")\n",
    "\n",
    "print(f\"Unique tax_code_areas in parcel_civic_df: {parcel_civic_df['tax_code_area'].nunique()}\")\n",
    "print(f\"Unique tax_code_areas in all_levies_df: {all_levies_df['tax_code_area'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all levy_names that include 'SD081'\n",
    "sd081_levies = all_levies_df[all_levies_df['levy_name'].str.contains('SD081', case=False, na=False)]\n",
    "\n",
    "if sd081_levies.empty:\n",
    "    print(\"No levies found including 'SD081'.\")\n",
    "else:\n",
    "    print(\"Summary statistics for levies including 'SD081':\")\n",
    "    for levy in sd081_levies['levy_name'].unique():\n",
    "        levy_rows = sd081_levies[sd081_levies['levy_name'] == levy]\n",
    "        for col in ['levy_rate_24', 'levy_rate_25']:\n",
    "            if col in levy_rows.columns:\n",
    "                # Convert to numeric, coerce errors to NaN\n",
    "                vals = pd.to_numeric(levy_rows[col], errors='coerce').dropna()\n",
    "                if not vals.empty:\n",
    "                    min_val = vals.min()\n",
    "                    median_val = vals.median()\n",
    "                    max_val = vals.max()\n",
    "                    print(f\"Levy: {levy} | {col} - min: {min_val:.6f}, median: {median_val:.6f}, max: {max_val:.6f}\")\n",
    "                else:\n",
    "                    print(f\"Levy: {levy} | {col} - No data\")\n",
    "\n",
    "# Do the same for any levy including 'county'\n",
    "county_levies = all_levies_df[all_levies_df['levy_name'].str.contains('county', case=False, na=False)]\n",
    "\n",
    "if county_levies.empty:\n",
    "    print(\"No levies found including 'county'.\")\n",
    "else:\n",
    "    print(\"Summary statistics for levies including 'county':\")\n",
    "    for levy in county_levies['levy_name'].unique():\n",
    "        levy_rows = county_levies[county_levies['levy_name'] == levy]\n",
    "        for col in ['levy_rate_24', 'levy_rate_25']:\n",
    "            if col in levy_rows.columns:\n",
    "                # Convert to numeric, coerce errors to NaN\n",
    "                vals = pd.to_numeric(levy_rows[col], errors='coerce').dropna()\n",
    "                if not vals.empty:\n",
    "                    min_val = vals.min()\n",
    "                    median_val = vals.median()\n",
    "                    max_val = vals.max()\n",
    "                    print(f\"Levy: {levy} | {col} - min: {min_val:.6f}, median: {median_val:.6f}, max: {max_val:.6f}\")\n",
    "                else:\n",
    "                    print(f\"Levy: {levy} | {col} - No data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Filtering to Spokane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to Spokane properties only\n",
    "df = parcel_civic_df\n",
    "\n",
    "print(f\"Filtered to {len(df):,} Spokane properties\")\n",
    "print(f\"Original dataset had {len(parcel_civic_df):,} total properties\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.5 Add in Millage Rates by District\n",
    "\n",
    "Spokane would add an abatement to buildings which would apply across taxing districts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all levy_names for tax_code_area 0011\n",
    "levy_names_0011 = all_levies_df.loc[all_levies_df['tax_code_area'] == \"0011\", 'levy_name'].unique()\n",
    "print(\"Levy names for tax_code_area 0011:\")\n",
    "for name in levy_names_0011:\n",
    "    print(name)\n",
    "\n",
    "# Show the full table with all columns for tax_code_area 0011\n",
    "levies_0011_df = all_levies_df.loc[all_levies_df['tax_code_area'] == \"0011\"]\n",
    "print(\"Full table for tax_code_area 0011:\")\n",
    "print(levies_0011_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove levy_name = \"Totals:\" from all_levies_df\n",
    "all_levies_df = all_levies_df[all_levies_df['levy_name'] != \"Totals:\"]\n",
    "print(f\"Removed 'Totals:' entries. Remaining records: {len(all_levies_df):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total count of all millages in all_levies_df: {len(all_levies_df):,}\")\n",
    "print(f\"Median levy_rate_25 by levy name (excluding voted levies):\")\n",
    "# Filter out voted levies before calculating medians\n",
    "non_voted_levies = all_levies_df[all_levies_df['levy_type'] != \"Voted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure numeric\n",
    "all_levies_df['levy_rate_25'] = pd.to_numeric(all_levies_df['levy_rate_25'], errors='coerce').fillna(0.0)\n",
    "\n",
    "# --- Identify the Spokane-General tax code areas (TCA) ---\n",
    "spokane_name = 'spokane general'\n",
    "spokane_mask = all_levies_df['levy_name'].str.casefold().eq(spokane_name)\n",
    "spokane_tcas = set(all_levies_df.loc[spokane_mask, 'tax_code_area'].dropna().unique())\n",
    "\n",
    "if not spokane_tcas:\n",
    "    raise ValueError(\"No tax_code_area contains 'Spokane General' in levy_name. Check your data/labels.\")\n",
    "\n",
    "# --- Build levy_name -> set(TCA) mapping (use the same universe as your analysis; here we include all rows) ---\n",
    "levy_to_tcas = (\n",
    "    all_levies_df\n",
    "    .groupby('levy_name')['tax_code_area']\n",
    "    .apply(lambda s: set(s.dropna().unique()))\n",
    ")\n",
    "\n",
    "# Only levies that appear exclusively in Spokane-General TCAs (no outside TCAs)\n",
    "only_in_spokane = levy_to_tcas.apply(lambda s: len(s) > 0 and s.issubset(spokane_tcas))\n",
    "\n",
    "# Bucket 1: present in every Spokane-General TCA (and nowhere else)\n",
    "bucket1_names = levy_to_tcas[only_in_spokane].index[\n",
    "    levy_to_tcas[only_in_spokane].apply(lambda s: s == spokane_tcas)\n",
    "].tolist()\n",
    "\n",
    "# Bucket 2: present in some (≥1) but not all Spokane-General TCAs (and nowhere else)\n",
    "bucket2_names = levy_to_tcas[only_in_spokane].index[\n",
    "    levy_to_tcas[only_in_spokane].apply(lambda s: (s != spokane_tcas) and (len(s) >= 1))\n",
    "].tolist()\n",
    "\n",
    "# Exclude \"Spokane General\" itself from both buckets\n",
    "bucket1_names = [n for n in bucket1_names if n.strip().lower() != spokane_name]\n",
    "bucket2_names = [n for n in bucket2_names if n.strip().lower() != spokane_name]\n",
    "\n",
    "# --- Check SD081 Spokane General specifically ---\n",
    "sd081_name = 'SD081 Spokane General'\n",
    "if sd081_name in levy_to_tcas.index:\n",
    "    sd081_tcas = levy_to_tcas[sd081_name]\n",
    "    sd081_outside_spokane = sd081_tcas - spokane_tcas\n",
    "    print(f\"SD081 Spokane General appears in {len(sd081_tcas)} total TCAs\")\n",
    "    print(f\"SD081 Spokane General appears in {len(sd081_outside_spokane)} TCAs outside of Spokane General\")\n",
    "    if sd081_outside_spokane:\n",
    "        print(f\"Extra TCAs: {sorted(sd081_outside_spokane)}\")\n",
    "else:\n",
    "    print(\"SD081 Spokane General not found in levy data\")\n",
    "\n",
    "# --- Helper to summarize levy \"millages\" within Spokane-General TCAs ---\n",
    "def summarize_levies(names):\n",
    "    if not names:\n",
    "        return pd.DataFrame(columns=['levy_name','levy_type','tca_count','mean_millage','min_millage','max_millage','total_millage'])\n",
    "    sub = all_levies_df[\n",
    "        all_levies_df['levy_name'].isin(names)\n",
    "        & all_levies_df['tax_code_area'].isin(spokane_tcas)\n",
    "    ]\n",
    "    # Get the most common levy_type for each levy_name (in case of inconsistencies)\n",
    "    type_map = (\n",
    "        sub.groupby('levy_name')['levy_type']\n",
    "        .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else x.iloc[0] if len(x) else None)\n",
    "    )\n",
    "    summary = (\n",
    "        sub.groupby('levy_name')\n",
    "           .agg(\n",
    "               tca_count=('tax_code_area', 'nunique'),\n",
    "               mean_millage=('levy_rate_25', 'mean'),\n",
    "               min_millage=('levy_rate_25', 'min'),\n",
    "               max_millage=('levy_rate_25', 'max'),\n",
    "               total_millage=('levy_rate_25', 'sum')\n",
    "           )\n",
    "           .sort_values('levy_name')\n",
    "           .reset_index()\n",
    "    )\n",
    "    summary['levy_type'] = summary['levy_name'].map(type_map)\n",
    "    # Move levy_type to after levy_name\n",
    "    cols = ['levy_name', 'levy_type', 'tca_count', 'mean_millage', 'min_millage', 'max_millage', 'total_millage']\n",
    "    summary = summary[cols]\n",
    "    return summary\n",
    "\n",
    "# --- Helper to compute per-TCA total millage sums for a bucket (i.e., \"total millages\" by TCA) ---\n",
    "def per_tca_totals(names):\n",
    "    if not names:\n",
    "        return pd.DataFrame(columns=['tax_code_area','bucket_total_millage'])\n",
    "    sub = all_levies_df[\n",
    "        all_levies_df['levy_name'].isin(names)\n",
    "        & all_levies_df['tax_code_area'].isin(spokane_tcas)\n",
    "    ]\n",
    "    return (\n",
    "        sub.groupby('tax_code_area', as_index=False)['levy_rate_25']\n",
    "           .sum()\n",
    "           .rename(columns={'levy_rate_25': 'bucket_total_millage'})\n",
    "           .sort_values('tax_code_area')\n",
    "    )\n",
    "\n",
    "# --- Compute outputs ---\n",
    "bucket1_levy_summary = summarize_levies(bucket1_names)\n",
    "bucket2_levy_summary = summarize_levies(bucket2_names)\n",
    "\n",
    "bucket1_per_tca = per_tca_totals(bucket1_names)\n",
    "bucket2_per_tca = per_tca_totals(bucket2_names)\n",
    "\n",
    "# --- Print results ---\n",
    "print(\"Bucket 1 — In every Spokane-General TCA and nowhere else\")\n",
    "print(f\"Levy count: {len(bucket1_names)}\")\n",
    "if not bucket1_levy_summary.empty:\n",
    "    print(\"Levy summary (including whether voted or not):\")\n",
    "    print(bucket1_levy_summary.to_string(index=False))\n",
    "else:\n",
    "    print(\"No levies in this bucket.\")\n",
    "print(\"\\nPer–TCA totals (Bucket 1):\")\n",
    "print(bucket1_per_tca.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Bucket 2 — In some Spokane-General TCAs and nowhere else\")\n",
    "print(f\"Levy count: {len(bucket2_names)}\")\n",
    "if not bucket2_levy_summary.empty:\n",
    "    print(\"Levy summary (including whether voted or not):\")\n",
    "    print(bucket2_levy_summary.to_string(index=False))\n",
    "else:\n",
    "    print(\"No levies in this bucket.\")\n",
    "print(\"\\nPer–TCA totals (Bucket 2):\")\n",
    "print(bucket2_per_tca.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Convenience: normalize the levy_name to safely run regex tests\n",
    "name = all_levies_df['levy_name'].fillna(\"\")\n",
    "\n",
    "# 1% (constitutional) flag:\n",
    "# - regular (non-voted) levies count\n",
    "# - except Port/PUD regular levies (excluded by constitution)\n",
    "is_non_voted = all_levies_df['levy_type'].ne(\"Voted\")\n",
    "is_port_or_pud = name.str.contains(r'\\b(PUD|Public Utility District|Port)\\b', flags=re.I, na=False)\n",
    "\n",
    "all_levies_df['one_cap_flag'] = is_non_voted & ~is_port_or_pud\n",
    "\n",
    "# $5.90 (statutory) bucket flag:\n",
    "# Start from non-voted, then exclude categories that sit outside $5.90:\n",
    "# - State School (both lines)\n",
    "# - EMS regular levies\n",
    "# - Conservation Futures\n",
    "# - Port/PUD regular levies\n",
    "is_state_school = name.str.contains(r'\\bState\\s+School(\\s+Levy\\s*2)?\\b', flags=re.I, na=False)\n",
    "\n",
    "is_ems = name.str.contains(r'\\bEMS\\b|\\bEmergency\\s+Medical', flags=re.I, na=False)\n",
    "\n",
    "is_conservation_futures = name.str.contains(r'\\bCons(?:ervation)?\\s+Futures\\b', flags=re.I, na=False)\n",
    "\n",
    "all_levies_df['in_590_bucket'] = (\n",
    "    is_non_voted\n",
    "    & ~is_state_school\n",
    "    & ~is_ems\n",
    "    & ~is_conservation_futures\n",
    "    & ~is_port_or_pud\n",
    ")\n",
    "\n",
    "# (Optional) Quick sanity check summaries:\n",
    "print(\"Counts → one_cap_flag:\")\n",
    "print(all_levies_df.groupby('one_cap_flag').size())\n",
    "print(\"\\nCounts → in_590_bucket (subset of non-voted):\")\n",
    "print(all_levies_df[is_non_voted].groupby('in_590_bucket').size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if \"Spokane Bond\" is a voted levy\n",
    "spokane_bond_rows = all_levies_df[all_levies_df['levy_name'].str.contains('spokane bond', case=False, na=False)]\n",
    "if not spokane_bond_rows.empty:\n",
    "    print(\"Spokane Bond 'levy_type' values:\")\n",
    "    print(spokane_bond_rows['levy_type'].value_counts())\n",
    "    is_voted = (spokane_bond_rows['levy_type'].str.lower() == 'voted').any()\n",
    "    print(f\"\\nIs Spokane Bond voted? {'Yes' if is_voted else 'No'}\")\n",
    "else:\n",
    "    print(\"No rows found for 'Spokane Bond' in all_levies_df.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all rows where the levy_name contains 'SD081' (case-insensitive)\n",
    "sd081_rows = all_levies_df[all_levies_df['levy_name'].str.contains('SD081', case=False, na=False)]\n",
    "\n",
    "# Get unique combinations of levy_name, levy_rate_25, and levy_type (to show voted/non-voted)\n",
    "unique_sd081_millages = (\n",
    "    sd081_rows[['levy_name', 'levy_rate_25', 'levy_type']]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(['levy_name', 'levy_rate_25'])\n",
    ")\n",
    "\n",
    "print(\"Unique millages containing 'SD081' and whether they are voted or not:\")\n",
    "print(unique_sd081_millages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Set pandas display option to ensure all rows are visible when displaying in notebook\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(\"Levy name counts:\")\n",
    "display(all_levies_df['levy_name'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- Construct per-levy flags DataFrame (levy_df) ---\n",
    "all_levies_df['levy_rate_25'] = pd.to_numeric(all_levies_df['levy_rate_25'], errors='coerce').fillna(0.0)\n",
    "nm = all_levies_df['levy_name'].fillna('')\n",
    "\n",
    "# one_cap_flag: True for regular (non-voted) levies except Port/PUD\n",
    "is_port_or_pud = nm.str.contains(r'\\b(?:PUD|Public Utility District|Port)\\b', flags=re.I, na=False)\n",
    "all_levies_df['one_cap_flag'] = all_levies_df['levy_type'].ne(\"Voted\") & ~is_port_or_pud\n",
    "\n",
    "# statute_flag: non-voted regulars (not state school, EMS, conservation futures, or port/pud)\n",
    "is_state_school = nm.str.contains(r'\\bState\\s+School(?:\\s+Levy\\s*2)?\\b', flags=re.I, na=False)\n",
    "is_ems = nm.str.contains(r'\\bEMS\\b|\\bEmergency\\s+Medical', flags=re.I, na=False)\n",
    "is_consfut = nm.str.contains(r'\\bCons(?:ervation)?\\s+Futures\\b', flags=re.I, na=False)\n",
    "is_non_voted = all_levies_df['levy_type'].ne(\"Voted\")\n",
    "all_levies_df['statute_flag'] = (\n",
    "    is_non_voted & ~is_state_school & ~is_ems & ~is_consfut & ~is_port_or_pud\n",
    ")\n",
    "\n",
    "is_city_general = nm.str.contains(r'\\bspokane\\s+general\\b', flags=re.I, na=False)\n",
    "is_county_general = nm.str.contains(r'\\bcounty\\s+general\\b', flags=re.I, na=False)\n",
    "is_county_road = nm.str.contains(r'\\bcounty\\s+road\\b', flags=re.I, na=False)\n",
    "all_levies_df['statute_360_flag'] = (\n",
    "    all_levies_df['statute_flag'] &\n",
    "    (is_city_general | is_county_general | is_county_road)\n",
    ")\n",
    "all_levies_df['statute_590_flag'] = all_levies_df['statute_flag']\n",
    "\n",
    "# --- Add in_spokane_boundaries flag ---\n",
    "_spokane_name = \"spokane general\"\n",
    "_spokane_mask = all_levies_df['levy_name'].str.casefold().eq(_spokane_name)\n",
    "_spokane_tcas = set(all_levies_df.loc[_spokane_mask, 'tax_code_area'].dropna().unique())\n",
    "if not _spokane_tcas:\n",
    "    raise ValueError(\"No tax_code_area contains 'Spokane General' in levy_name. Check labels/spelling.\")\n",
    "\n",
    "_levy_to_tcas = (\n",
    "    all_levies_df.groupby('levy_name')['tax_code_area']\n",
    "    .apply(lambda s: set(s.dropna().unique()))\n",
    ")\n",
    "\n",
    "only_in_spokane_levy_mask = _levy_to_tcas.apply(lambda s: len(s) > 0 and s.issubset(_spokane_tcas))\n",
    "_spokane_only_levies = _levy_to_tcas[only_in_spokane_levy_mask].index.tolist()\n",
    "_sd081_extra_levies = [\"SD081 Spokane B&I\", \"SD081 Spokane General\"]\n",
    "for extra_levy in _sd081_extra_levies:\n",
    "    if extra_levy not in _spokane_only_levies and extra_levy in all_levies_df['levy_name'].unique():\n",
    "        _spokane_only_levies.append(extra_levy)\n",
    "spokane_boundaries_set = set(_spokane_only_levies)\n",
    "\n",
    "# summarize per-levy-name for flags and in_spokane_boundaries\n",
    "levy_df = (\n",
    "    all_levies_df\n",
    "    .groupby('levy_name', dropna=False)\n",
    "    .agg(\n",
    "        total_levy_rate_25=('levy_rate_25', 'first'),\n",
    "        any_one_cap_flag=('one_cap_flag', 'any'),\n",
    "        any_statute_flag=('statute_flag', 'any'),\n",
    "        any_statute_590_flag=('statute_590_flag', 'any'),\n",
    "        any_statute_360_flag=('statute_360_flag', 'any'),\n",
    "        tca_set=('tax_code_area', lambda s: set(s.dropna().unique()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "levy_df['in_spokane_boundaries'] = levy_df['levy_name'].isin(spokane_boundaries_set)\n",
    "levy_df.to_csv(\"levy_df_flags_spokane.csv\", index=False)\n",
    "\n",
    "# --- 2. Create summary_by_tca: sum of total, one_cap, statute, statute_590, statute_360, and spokane_general for each TCA\n",
    "all_levies_df['_rate_all']      = all_levies_df['levy_rate_25']\n",
    "all_levies_df['_rate_one_cap']  = np.where(all_levies_df['one_cap_flag'],     all_levies_df['levy_rate_25'], 0.0)\n",
    "all_levies_df['_rate_statute']  = np.where(all_levies_df['statute_flag'],     all_levies_df['levy_rate_25'], 0.0)\n",
    "all_levies_df['_rate_statute_590']  = np.where(all_levies_df['statute_590_flag'], all_levies_df['levy_rate_25'], 0.0)\n",
    "all_levies_df['_rate_statute_360']  = np.where(all_levies_df['statute_360_flag'], all_levies_df['levy_rate_25'], 0.0)\n",
    "\n",
    "summary_by_tca = (\n",
    "    all_levies_df\n",
    "    .groupby('tax_code_area', as_index=False)\n",
    "    .agg(\n",
    "        total_millage   = ('_rate_all',     'sum'),\n",
    "        one_cap_millage = ('_rate_one_cap', 'sum'),\n",
    "        statute_millage = ('_rate_statute', 'sum'),\n",
    "        statute_590_millage = ('_rate_statute_590', 'sum'),\n",
    "        statute_360_millage = ('_rate_statute_360', 'sum'),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add Spokane General millage per TCA\n",
    "spokane_general = (\n",
    "    all_levies_df.loc[all_levies_df['levy_name'].str.casefold() == 'spokane general']\n",
    "    .groupby('tax_code_area', as_index=False)['levy_rate_25']\n",
    "    .sum()\n",
    "    .rename(columns={'levy_rate_25': 'spokane_general_millage'})\n",
    ")\n",
    "summary_by_tca = summary_by_tca.merge(spokane_general, on='tax_code_area', how='left')\n",
    "summary_by_tca['spokane_general_millage'] = summary_by_tca['spokane_general_millage'].fillna(0.0)\n",
    "\n",
    "# Add total millage with only-in-spokane-boundary levies per tca\n",
    "_spokane_only_rows = all_levies_df[\n",
    "    all_levies_df['levy_name'].isin(spokane_boundaries_set)\n",
    "    & all_levies_df['tax_code_area'].isin(_spokane_tcas)\n",
    "].copy()\n",
    "\n",
    "_spokane_boundaries_per_tca = (\n",
    "    _spokane_only_rows\n",
    "    .groupby('tax_code_area', as_index=False)['levy_rate_25']\n",
    "    .sum()\n",
    "    .rename(columns={'levy_rate_25': 'total_spokane_boundaries_millage'})\n",
    ")\n",
    "\n",
    "summary_by_tca = summary_by_tca.merge(_spokane_boundaries_per_tca, on='tax_code_area', how='left')\n",
    "summary_by_tca['total_spokane_boundaries_millage'] = summary_by_tca['total_spokane_boundaries_millage'].fillna(0.0)\n",
    "\n",
    "# --- 3. Make per-levy millage columns in df for every unique levy ---\n",
    "\n",
    "# Create a tax_code_area x levy_name pivot of millages (fill missing with 0)\n",
    "tca_levy_millage = all_levies_df.pivot_table(\n",
    "    index='tax_code_area',\n",
    "    columns='levy_name',\n",
    "    values='levy_rate_25',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0.0\n",
    ")\n",
    "\n",
    "# For each parcel, add a column for each levy: levy_millage_(levyname)\n",
    "df = df.copy()\n",
    "levyname_list = list(tca_levy_millage.columns)\n",
    "for levyname in levyname_list:\n",
    "    colname = \"levy_millage_\" + str(levyname).strip().lower().replace(' ', '_').replace('&', 'and').replace('/', '_').replace('-', '_').replace('.', '_')\n",
    "    millage_by_tca = tca_levy_millage[levyname]\n",
    "    df = df.merge(\n",
    "        millage_by_tca.rename(colname).reset_index(),\n",
    "        on='tax_code_area',\n",
    "        how='left'\n",
    "    )\n",
    "    df[colname] = df[colname].fillna(0.0)\n",
    "\n",
    "# -- Optionally print --\n",
    "print(\"levy_df (first 10 rows):\")\n",
    "print(levy_df.head(10))\n",
    "print(\"\\nsummary_by_tca (first 10 rows):\")\n",
    "print(summary_by_tca.head(10))\n",
    "colprint = [c for c in df.columns if c.startswith('levy_millage_')]\n",
    "print(\"\\nParcel levy millage columns (first 5 rows):\")\n",
    "print(df[['tax_code_area'] + colprint].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate min, max, and median for statute_590_millage and statute_360_millage from levy_summary\n",
    "for col in ['statute_590_millage', 'statute_360_millage']:\n",
    "    if col in summary_by_tca.columns:\n",
    "        col_min = summary_by_tca[col].min()\n",
    "        col_max = summary_by_tca[col].max()\n",
    "        col_median = summary_by_tca[col].median()\n",
    "        print(f\"{col}: min={col_min}, max={col_max}, median={col_median}\")\n",
    "    else:\n",
    "        print(f\"Column '{col}' not found in levy_summary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to levy_df indicating the levy_millage_* column name used in df\n",
    "def normalize_levy_name(name: str) -> str:\n",
    "    return (\n",
    "        str(name)\n",
    "        .strip()\n",
    "        .lower()\n",
    "        .replace(' ', '_')\n",
    "        .replace('&', 'and')\n",
    "        .replace('/', '_')\n",
    "        .replace('-', '_')\n",
    "        .replace('.', '_')\n",
    "    )\n",
    "\n",
    "levy_df['millage_col'] = levy_df['levy_name'].apply(\n",
    "    lambda x: f\"levy_millage_{normalize_levy_name(x)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all tax code areas where Spokane General levy appears\n",
    "\n",
    "spokane_general_levy_mask = all_levies_df['levy_name'].str.casefold() == 'spokane general'\n",
    "spokane_general_tcas = all_levies_df.loc[spokane_general_levy_mask, 'tax_code_area'].unique()\n",
    "print(\"Tax code areas with 'Spokane General' levy:\")\n",
    "for tca in sorted(spokane_general_tcas):\n",
    "    print(tca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "levy_statute_summary = (\n",
    "    all_levies_df\n",
    "    .groupby('levy_name', dropna=False)\n",
    "    .agg(\n",
    "        statute_flag=('statute_flag', 'first'),\n",
    "        statute_360_flag=('statute_360_flag', 'first'),\n",
    "        statute_590_flag=('statute_590_flag', 'first'),\n",
    "        median_levy_rate_25=('levy_rate_25', 'median')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "levy_statute_summary.to_csv(\"levy_statute_summary_spokane.csv\", index=False)\n",
    "# For each levy_name, check if all rows have the same levy_rate_25\n",
    "levyname_rate_varies = (\n",
    "    all_levies_df\n",
    "    .groupby('levy_name')['levy_rate_25']\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    "    .rename(columns={'levy_rate_25': 'n_unique_rates'})\n",
    ")\n",
    "\n",
    "# Add a column: is_same_rate (True if only 1 unique rate for that name)\n",
    "levyname_rate_varies['is_same_rate'] = levyname_rate_varies['n_unique_rates'] == 1\n",
    "\n",
    "# Compute the percent of levy_names where all rows have the same rate\n",
    "num_levy_names = len(levyname_rate_varies)\n",
    "num_same_rate = levyname_rate_varies['is_same_rate'].sum()\n",
    "percent_same_rate = (num_same_rate / num_levy_names) * 100 if num_levy_names > 0 else 0\n",
    "\n",
    "print(f\"{percent_same_rate:.2f}% of levy_names have the same levy_rate_25 for every row of that name \"\n",
    "      f\"({num_same_rate} of {num_levy_names})\")\n",
    "\n",
    "# Optionally, print a few examples where the rate varies\n",
    "varying = levyname_rate_varies[~levyname_rate_varies['is_same_rate']]\n",
    "if not varying.empty:\n",
    "    print(\"\\nlevy_names where levy_rate_25 is NOT the same for every row:\")\n",
    "    print(varying.head(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Recreating Current Property Tax Revenue\n",
    "\n",
    "Before we can model an LVT shift, we need to accurately recreate the current property tax system. This validation step ensures our dataset correctly reflects the real-world tax landscape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percent of rows in df where total_spokane_boundaries_millage is NA\n",
    "total = len(df)\n",
    "num_exmp_ge_assessed = (df['exmp_amt'] >= df['assessed_amt']).sum()\n",
    "percent_exmp_ge_assessed = (num_exmp_ge_assessed / len(df)) * 100 if len(df) > 0 else 0\n",
    "print(f\"Percent of rows with exmp_amt >= assessed_amt: {percent_exmp_ge_assessed:.2f}% ({num_exmp_ge_assessed} of {len(df)})\")\n",
    "# Calculate and print percent of exmp_amt and assessed_amt that are NA\n",
    "num_exmp_amt_na = df['exmp_amt'].isna().sum()\n",
    "num_assessed_amt_na = df['assessed_amt'].isna().sum()\n",
    "percent_exmp_amt_na = (num_exmp_amt_na / total) * 100 if total > 0 else 0\n",
    "percent_assessed_amt_na = (num_assessed_amt_na / total) * 100 if total > 0 else 0\n",
    "print(f\"Percent of rows with exmp_amt NA: {percent_exmp_amt_na:.2f}% ({num_exmp_amt_na} of {total})\")\n",
    "print(f\"Percent of rows with assessed_amt NA: {percent_assessed_amt_na:.2f}% ({num_assessed_amt_na} of {total})\")\n",
    "\n",
    "df['exmp_amt'] = df['exmp_amt'].fillna(0)\n",
    "\n",
    "# Restrict to rows with non-NA spokane_general_millage and assessed_amt > exmp_amt, print percent removed\n",
    "num_rows_before = len(df)\n",
    "df = df[(df['levy_millage_spokane_general'].notna()) & (df['levy_millage_spokane_general'] != 0)] \n",
    "df = df[df['assessed_amt'] > df['exmp_amt']]\n",
    "num_rows_after = len(df)\n",
    "percent_removed = ((num_rows_before - num_rows_after) / num_rows_before) * 100 if num_rows_before > 0 else 0\n",
    "print(f\"Removed {num_rows_before - num_rows_after} rows ({percent_removed:.2f}%) where total_spokane_boundaries_millage was NA or 0, or assessed_amt <= exmp_amt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns that start with 'levy_millage_' where all entries are either NA or 0\n",
    "levy_millage_cols = [col for col in df.columns if col.startswith('levy_millage_')]\n",
    "cols_to_drop = [col for col in levy_millage_cols if df[col].isna().all() or (df[col].fillna(0) == 0).all()]\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "print(f\"Removed {len(cols_to_drop)} levy_millage_ columns that were all NA or 0: {cols_to_drop}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate current tax for total_spokane_boundaries_millage and save as current_tax ---\n",
    "df['millage_rate'] = pd.to_numeric(df['levy_millage_spokane_general'], errors='coerce')\n",
    "print(f\"Median millage_rate: {df['millage_rate'].median()}\")\n",
    "\n",
    "# Set exmp_flag = 1 if exmp_amt > assessed_amt or taxable_amt <= 0.05, else 0\n",
    "df['exmp_flag'] = ((df['exmp_amt'] > df['assessed_amt']) | (df['taxable_amt'] <= 0.05)).astype(int)\n",
    "df = df[df['exmp_flag'] != 1].copy()\n",
    "# Calculate current revenue for the overall Spokane boundaries millage using calculate_current_tax\n",
    "# This will also save the per-row current_tax to df['current_tax']\n",
    "current_revenue, second_revenue, df = calculate_current_tax(\n",
    "    df=df, \n",
    "    tax_value_col='assessed_amt',\n",
    "    millage_rate_col='millage_rate',\n",
    "    exemption_col='exmp_amt',\n",
    "    exemption_flag_col='exmp_flag'\n",
    ")\n",
    "\n",
    "print(f\"Total number of properties: {len(df):,}\")\n",
    "print(f\"Current annual revenue with millage rate: ${current_revenue:,.2f}\")\n",
    "print(f\"Total land value: ${df['land_value'].sum():,.2f}\")\n",
    "print(f\"Total overall value: ${df['assessed_amt'].sum():,.2f}\")\n",
    "print(f\"Total taxable value: ${df['taxable_amt'].sum():,.2f}\")\n",
    "\n",
    "total_assessed_minus_exempt = (df['assessed_amt'] - df['exmp_amt']).sum()\n",
    "print(\"Sum of assessed_amt minus exmp_amt:\", total_assessed_minus_exempt)\n",
    "\n",
    "# Calculate millage_rate as (current_revenue / total_assessed_minus_exempt) * 1000\n",
    "millage_rate = (current_revenue / total_assessed_minus_exempt) * 1000\n",
    "print(\"Calculated millage_rate:\", millage_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate current tax for every levy millage column (those starting with 'levy_millage_')\n",
    "\n",
    "levy_millage_cols = [col for col in df.columns if col.startswith('levy_millage_')]\n",
    "\n",
    "levy_millage_revenues = {}\n",
    "current_tax_cols = []\n",
    "\n",
    "for millage_col in levy_millage_cols:\n",
    "    # Generate output column by removing 'levy_millage_' and adding '_current_tax'\n",
    "    suffix = millage_col[len('levy_millage_'):]\n",
    "    tax_col = f\"levy_millage_{suffix}_current_tax\"\n",
    "    current_tax_cols.append(tax_col)\n",
    "    \n",
    "    # Convert millage column to numeric\n",
    "    df[millage_col] = pd.to_numeric(df[millage_col], errors='coerce')\n",
    "    \n",
    "    # Calculate current tax for this levy millage column\n",
    "    revenue, _, df_mill = calculate_current_tax(\n",
    "        df=df.copy(),\n",
    "        tax_value_col='assessed_amt',\n",
    "        millage_rate_col=millage_col,\n",
    "        exemption_col='exmp_amt',\n",
    "        exemption_flag_col='exmp_flag'\n",
    "    )\n",
    "    # Save the per-row current_tax as the appropriate column in the main df\n",
    "    df[tax_col] = df_mill['current_tax']\n",
    "    levy_millage_revenues[millage_col] = revenue\n",
    "    print(f\"Current annual revenue for {millage_col}: ${revenue:,.2f}\")\n",
    "\n",
    "# Optionally, print the sum for all the levy millage columns\n",
    "total_levy_millage_revenue = sum(levy_millage_revenues.values())\n",
    "print(f\"Total current revenue from all levy millage columns: ${total_levy_millage_revenue:,.2f}\")\n",
    "\n",
    "# Ensure total_current_tax is the sum of all current tax columns\n",
    "df['total_current_tax'] = df[current_tax_cols].fillna(0).sum(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_property_type(prop_use_desc):\n",
    "    # Direct mapping based on the property use descriptions from the data\n",
    "    category_mapping = {\n",
    "        \"Single Family\": [\"Single Unit\"],\n",
    "        \"Small Multi-Family (2-4 units)\": [\"Two-to-Four Unit\"],\n",
    "        \"Large Multi-Family (5+ units)\": [\"Five-Plus Unit\"],\n",
    "        \"Other Residential\": [\"Other Residential\", \"Vacation Home\"],\n",
    "        \"Mobile Home Park\": [\"Mobile Home Park\"],\n",
    "        \"Vacant Land\": [\"Vacant Land\"],\n",
    "        \"Agricultural\": [\"Cur - Use - Ag\", \"Agricultural Not Classified\", \"Agricultural\"],\n",
    "        \"Retail/Service/Commercial\": [\n",
    "            \"Retail - General Mrchds\", \"Retail - Other\", \"Retail - Hardware\", \"Retail - Food\", \"Retail - Eating\",\n",
    "            \"Retail - Auto\", \"Retail - Furniture\", \"Service - Finance\", \"Service - Professional\", \"Service - Repair\",\n",
    "            \"Service - Education\", \"Service - Governmental\", \"Service - Construction\", \"Service - Personal\",\n",
    "            \"Service - Business\", \"Wholesale\", \"Hotel/Motel\", \"Hotel/Condo\", \"Inst Lodging\", \"Recreational\",\n",
    "            \"Resort - Camping\", \"Public Assembly\", \"Churches\", \"Park\", \"Other Cultural\"\n",
    "        ],\n",
    "        \"Manufacturing/Industrial\": [\n",
    "            \"Manf - Other\", \"Manf - Fabricated Material\", \"Manf - Petroleum\", \"Manf - Printed Material\",\n",
    "            \"Manf - Stone/Glass\", \"Manf - Printing\", \"Manf - Instrumentation\", \"Manf - Leather\", \"Manf - Paper\",\n",
    "            \"Manufacturing - Food\", \"Manufacturing - Lumber\", \"Mining\", \"Utilities\", \"Communication\"\n",
    "        ],\n",
    "        \"Transportation - Parking\": [\"Trans - Parking\"],\n",
    "        \"Transportation/Other\": [\n",
    "            \"Trans - Highway\", \"Trans - Railroad\", \"Trans - Aircraft\", \"Trans - Motor\", \"Trans - Other\"\n",
    "        ],\n",
    "        \"Designated Forest\": [\"Designated Forest Lnd\"],\n",
    "        \"Water Areas\": [\"Water Area\"],\n",
    "        \"Marijuana\": [\"Marijuana Growing\"],\n",
    "        \"Current Use Open\": [\"Cur - Use - Open\"]\n",
    "    }\n",
    "\n",
    "    # Check for exact matches first\n",
    "    for category, keywords in category_mapping.items():\n",
    "        if prop_use_desc in keywords:\n",
    "            return category\n",
    "\n",
    "    # If no match found, return \"Other\"\n",
    "    return \"Other\"\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df['PROPERTY_CATEGORY'] = df['prop_use_desc'].apply(categorize_property_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Modeling the Split-Rate Land Value Tax\n",
    "\n",
    "Now for the exciting part - modeling the LVT shift! We'll create a revenue-neutral policy that taxes land at 4 times the rate of buildings.\n",
    "\n",
    "### The Split-Rate Formula\n",
    "\n",
    "Under our proposed system:\n",
    "- **Buildings** are taxed at a lower rate (Building Millage)  \n",
    "- **Land** is taxed at 4x that rate (4 × Building Millage)\n",
    "- **Total revenue** remains the same as current system\n",
    "\n",
    "The formula to solve for the building millage rate is:\n",
    "```\n",
    "Current Revenue = (Building Millage × Total Taxable Buildings) + (4 × Building Millage × Total Taxable Land)\n",
    "```\n",
    "\n",
    "### Handling Exemptions in Split-Rate System\n",
    "\n",
    "Since we want to maintain existing exemptions, we need to:\n",
    "1. Apply exemptions to building value first\n",
    "2. If exemptions exceed building value, apply remainder to land value\n",
    "3. Calculate separate taxable values for land and buildings\n",
    "\n",
    "This ensures properties don't over-benefit from exemptions and maintains the intent of existing tax policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find properties where assessed_amt < land_value\n",
    "mask = df['assessed_amt'] < df['land_value']\n",
    "\n",
    "# Print value counts of prop_use_desc for those properties\n",
    "if 'prop_use_desc' in df.columns:\n",
    "    print(\"prop_use_desc counts where assessed_amt < land_value:\")\n",
    "    print(df.loc[mask, 'prop_use_desc'].value_counts())\n",
    "else:\n",
    "    print(\"Column 'prop_use_desc' not found in dataframe.\")\n",
    "\n",
    "# Set land_value to assessed_amt for those properties\n",
    "df.loc[mask, 'land_value'] = df.loc[mask, 'assessed_amt']\n",
    "\n",
    "# Check if any assessed_amt values are negative\n",
    "num_negative_assessed = (df['land_value'] < 0).sum()\n",
    "if num_negative_assessed > 0:\n",
    "    print(f\"Warning: There are {num_negative_assessed} properties with negative assessed_amt values.\")\n",
    "else:\n",
    "    print(\"No negative assessed_amt values found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Calculate split-rate (LVT) _new_tax for every levy millage column individually\n",
    "# with different parameters based on 590/360 flags\n",
    "# ===============================\n",
    "\n",
    "# Find all levy millage columns\n",
    "levy_millage_cols = [col for col in df.columns if col.startswith('levy_millage_')]\n",
    "\n",
    "# Calculate improvement_value as assessed_amt - land_value\n",
    "df['improvement_value'] = (df['assessed_amt'] - df['land_value']).clip(lower=0)\n",
    "\n",
    "# For results\n",
    "levy_millage_rates = {}      # split-rate per levy\n",
    "levy_new_revenues = {}       # modeled revenue per levy\n",
    "levy_current_revenues = {}   # current revenue per levy\n",
    "\n",
    "# ---- Build lookup from millage_col -> flags from levy_df ----\n",
    "levy_flag_lookup = (\n",
    "    levy_df[['millage_col', 'any_statute_590_flag', 'any_statute_360_flag']]\n",
    "    .fillna(False)\n",
    "    .set_index('millage_col')\n",
    "    .to_dict(orient='index')\n",
    ")\n",
    "# Example of levy_flag_lookup:\n",
    "# {\n",
    "#   'levy_millage_spokane_general': {'any_statute_590_flag': True, 'any_statute_360_flag': True},\n",
    "#   'levy_millage_sd081_spokane_bi': {'any_statute_590_flag': False, 'any_statute_360_flag': False},\n",
    "#   ...\n",
    "# }\n",
    "\n",
    "# Add _new_tax column for each levy\n",
    "for millage_col in levy_millage_cols:\n",
    "    # The corresponding _current_tax column (already calculated earlier)\n",
    "    suffix = millage_col[len(\"levy_millage_\"):]\n",
    "    tax_col = f\"levy_millage_{suffix}_current_tax\"\n",
    "    new_tax_col = f\"{millage_col}_new_tax\"\n",
    "    new_millage_col = f\"{millage_col}_new_millage\"\n",
    "\n",
    "    # Require the _current_tax column to exist for current revenue\n",
    "    if tax_col not in df.columns:\n",
    "        print(f\"Skipping {millage_col}: missing {tax_col} column for current revenue summing.\")\n",
    "        continue\n",
    "\n",
    "    # --- Choose parameters based on 590/360 flags ---\n",
    "    flags = levy_flag_lookup.get(\n",
    "        millage_col,\n",
    "        {'any_statute_590_flag': False, 'any_statute_360_flag': False}\n",
    "    )\n",
    "    has_590_or_360 = flags['any_statute_590_flag'] or flags['any_statute_360_flag']\n",
    "\n",
    "    if has_590_or_360:\n",
    "        # For 590 / 360 levies: 25% improvement exemption, $10k base\n",
    "        improvement_exemption_pct = 0.20\n",
    "        building_abatement_floor = 0\n",
    "    else:\n",
    "        # All other levies: 90% improvement exemption, $100k base\n",
    "        improvement_exemption_pct = 0.6\n",
    "        building_abatement_floor = 100000\n",
    "\n",
    "    current_revenue = df[tax_col].sum()\n",
    "    levy_current_revenues[millage_col] = current_revenue\n",
    "\n",
    "    # Model split-rate for this levy with its chosen parameters\n",
    "    millage_rate, modeled_revenue, df_result = model_stacking_improvement_exemption(\n",
    "        df,\n",
    "        land_value_col='land_value',\n",
    "        improvement_value_col='improvement_value',\n",
    "        current_revenue=current_revenue,\n",
    "        building_abatement_floor=building_abatement_floor,\n",
    "        improvement_exemption_percentage=improvement_exemption_pct,\n",
    "        exemption_col='exmp_amt',\n",
    "        exemption_flag_col='exmp_flag'\n",
    "    )\n",
    "\n",
    "    df[new_tax_col] = df_result[\"new_tax\"]\n",
    "    df[new_millage_col] = millage_rate\n",
    "    levy_millage_rates[millage_col] = millage_rate\n",
    "    levy_new_revenues[millage_col] = modeled_revenue\n",
    "\n",
    "    print(\n",
    "        f\"{millage_col}: flags(590={flags['any_statute_590_flag']}, \"\n",
    "        f\"360={flags['any_statute_360_flag']}), \"\n",
    "        f\"improv_exempt={improvement_exemption_pct:.2%}, \"\n",
    "        f\"floor=${building_abatement_floor:,.0f}, \"\n",
    "        f\"millage={millage_rate:.5f}, \"\n",
    "        f\"revenue=${modeled_revenue:,.2f}\"\n",
    "    )\n",
    "\n",
    "# Calculate and print totals\n",
    "total_new_splitrate_revenue = sum(levy_new_revenues.values())\n",
    "total_current_levy_revenue = sum(levy_current_revenues.values())\n",
    "print(f\"Total modeled split-rate revenue (sum of _new_tax): ${total_new_splitrate_revenue:,.2f}\")\n",
    "print(f\"Total current revenue (sum of _current_tax): ${total_current_levy_revenue:,.2f}\")\n",
    "print(f\"Difference: ${total_new_splitrate_revenue - total_current_levy_revenue:,.2f}\")\n",
    "\n",
    "# Build df_b with all _new_tax columns and a total\n",
    "df_b = df.copy()\n",
    "all_new_tax_cols = [f\"{col}_new_tax\" for col in levy_millage_cols if f\"{col}_new_tax\" in df.columns]\n",
    "df_b[\"total_new_tax\"] = df_b[all_new_tax_cols].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We'll compare the sum of current taxes to the sum of new modeled taxes for each pa\n",
    "\n",
    "# User manually selects calculation mode\n",
    "USE_ONLY_SPOKANE_BOUNDARIES = False # Set to True to use only Spokane boundaries levies for tax calculations\n",
    "\n",
    "if USE_ONLY_SPOKANE_BOUNDARIES:\n",
    "    # Find millage columns for levies in Spokane boundaries\n",
    "    in_spokane_levies = levy_df.loc[levy_df['in_spokane_boundaries'] == 1, 'millage_col'].dropna().tolist()\n",
    "    # Filter out any levies where the millage_col includes \"sd081\" (case insensitive)\n",
    "    in_spokane_levies = [col for col in in_spokane_levies if \"sd081\" not in col.lower()]\n",
    "    print(\"Using only Spokane boundary millages:\", in_spokane_levies)\n",
    "\n",
    "    new_tax_cols_in_spokane = [f\"{col}_new_tax\" for col in in_spokane_levies if f\"{col}_new_tax\" in df_b.columns]\n",
    "    current_tax_cols_in_spokane = [f\"{col}_current_tax\" for col in in_spokane_levies if f\"{col}_current_tax\" in df_b.columns]\n",
    "\n",
    "    df_b['new_tax'] = df_b[new_tax_cols_in_spokane].sum(axis=1) if new_tax_cols_in_spokane else 0\n",
    "    df_b['current_tax'] = df_b[current_tax_cols_in_spokane].sum(axis=1) if current_tax_cols_in_spokane else 0\n",
    "else:\n",
    "    # Default: use sum of all total levy columns\n",
    "    df_b['new_tax'] = df_b['total_new_tax']\n",
    "    df_b['current_tax'] = df_b['total_current_tax']\n",
    "\n",
    "df_b['tax_change'] = df_b['new_tax'] - df_b['current_tax']\n",
    "df_b['tax_change_pct'] = df_b['tax_change'] / df_b['current_tax'] * 100\n",
    "df_b['tax_change_pct'] = df_b['tax_change_pct'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "# Sum new_tax and current_tax, and output their values and difference\n",
    "total_new_tax = df_b['new_tax'].sum()\n",
    "total_current_tax = df_b['current_tax'].sum()\n",
    "print(f\"Total new_tax: ${total_new_tax:,.2f}\")\n",
    "print(f\"Total current_tax: ${total_current_tax:,.2f}\")\n",
    "print(f\"Difference (new - current): ${total_new_tax - total_current_tax:,.2f}\")\n",
    "\n",
    "# Save the full results table for further analysis\n",
    "output_full_results = df_b.copy()\n",
    "\n",
    "# Calculate and print the summary table for total tax impact (using lvt_utils)\n",
    "from lvt_utils import calculate_category_tax_summary, print_category_tax_summary\n",
    "\n",
    "# This will use the default 'PROPERTY_CATEGORY' column if present\n",
    "output_summary = calculate_category_tax_summary(\n",
    "    output_full_results,\n",
    "    category_col='PROPERTY_CATEGORY' if 'PROPERTY_CATEGORY' in output_full_results.columns else output_full_results.columns[0],  # fallback to first col if not present\n",
    "    current_tax_col='current_tax',\n",
    "    new_tax_col='new_tax'\n",
    ")\n",
    "print_category_tax_summary(output_summary, \"Total Tax Impact by Property Category (All sp_ Levies)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and report the sum of the absolute difference between current_tax and new_tax,\n",
    "# and what percent of the sum of current_tax that represents.\n",
    "\n",
    "# Calculate absolute difference per parcel\n",
    "df_b['abs_tax_diff'] = (df_b['current_tax'] - df_b['new_tax']).abs()\n",
    "\n",
    "# Sum absolute differences\n",
    "total_abs_tax_diff = df_b['abs_tax_diff'].sum()\n",
    "\n",
    "# Calculate what percent of total current tax that represents\n",
    "percent_of_current = (total_abs_tax_diff / total_current_tax) * 100 if total_current_tax != 0 else np.nan\n",
    "\n",
    "print(f\"Sum of absolute value of current_tax minus new_tax: ${total_abs_tax_diff:,.2f}\")\n",
    "print(f\"That is {percent_of_current:.2f}% of the sum of current_tax.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print every unique millage name in df_b and the resulting new millage\n",
    "\n",
    "# Find all columns in df_b that end with '_new_millage'\n",
    "millage_cols = [col for col in df_b.columns if col.endswith('_new_millage')]\n",
    "\n",
    "for col in millage_cols:\n",
    "    # Derive the millage name from the column name\n",
    "    # The format is expected: 'levy_millage_{NAME}_new_millage'\n",
    "    # or possibly '{NAME}_new_millage'\n",
    "    if col.startswith('levy_millage_'):\n",
    "        millage_name = col[len('levy_millage_'):-len('_new_millage')]\n",
    "    else:\n",
    "        millage_name = col[:-len('_new_millage')]\n",
    "    print(f\"\\nMillage name: '{millage_name}'\")\n",
    "    print(\"Unique new millage values:\")\n",
    "    print(df_b[col].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#  Sum NEW millage by tax_code_area, grouped by levy flags\n",
    "#  Using levy_df['millage_col'] instead of recomputing names\n",
    "# ============================================================\n",
    "\n",
    "records = []\n",
    "\n",
    "for _, row in levy_df.iterrows():\n",
    "    millage_col = f\"{row['millage_col']}_new_millage\"\n",
    "\n",
    "    if millage_col not in df_b.columns:\n",
    "        # print(f\"Missing millage column: {millage_col}\")\n",
    "        continue\n",
    "\n",
    "    temp = df_b[['tax_code_area', millage_col]].copy()\n",
    "    temp = temp.rename(columns={millage_col: 'new_millage'})\n",
    "    temp['levy_name'] = row['levy_name']\n",
    "\n",
    "    for flag_col in [\"any_statute_590_flag\", \"any_statute_360_flag\"]:\n",
    "        temp[flag_col] = row[flag_col]\n",
    "\n",
    "    records.append(temp)\n",
    "\n",
    "if not records:\n",
    "    raise ValueError(\"No *_new_millage levy columns found in df_b.\")\n",
    "\n",
    "long_df = pd.concat(records, ignore_index=True)\n",
    "\n",
    "# ---- Aggregate by tax_code_area ----\n",
    "summary_output = {}\n",
    "\n",
    "for flag in [\"any_statute_590_flag\", \"any_statute_360_flag\"]:\n",
    "    flagged = long_df[long_df[flag] == True]\n",
    "\n",
    "    # Summarize unique (tax_code_area, levy_name) new millages\n",
    "    tca_levy = (\n",
    "        flagged\n",
    "        .groupby(['tax_code_area', 'levy_name'], as_index=False)['new_millage']\n",
    "        .first()\n",
    "    )\n",
    "\n",
    "    # Sum millages across levies within the TCA\n",
    "    tca_sum = (\n",
    "        tca_levy\n",
    "        .groupby('tax_code_area')['new_millage']\n",
    "        .sum()\n",
    "    )\n",
    "\n",
    "    summary_output[flag] = {\n",
    "        \"min\": tca_sum.min(),\n",
    "        \"median\": tca_sum.median(),\n",
    "        \"max\": tca_sum.max(),\n",
    "        \"count_tcas\": len(tca_sum)\n",
    "    }\n",
    "\n",
    "# ---- Print results ----\n",
    "print(\"\\n============================================================\")\n",
    "print(\" NEW Millage Sums by Tax Code Area (using levy_df.millage_col)\")\n",
    "print(\"============================================================\")\n",
    "for flag, stats in summary_output.items():\n",
    "    print(f\"\\nFlag: {flag}\")\n",
    "    print(f\"  TCA count: {stats['count_tcas']}\")\n",
    "    print(f\"  Min:    {stats['min']:.6f}\")\n",
    "    print(f\"  Median: {stats['median']:.6f}\")\n",
    "    print(f\"  Max:    {stats['max']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Only include categories with property_count > 50\n",
    "filtered = output_summary[output_summary['property_count'] > 50].copy()\n",
    "\n",
    "categories = filtered['PROPERTY_CATEGORY'].tolist()\n",
    "counts = filtered['property_count'].tolist()\n",
    "median_pct_change = filtered['median_tax_change_pct'].tolist()\n",
    "median_dollar_change = filtered['median_tax_change'].tolist()\n",
    "total_tax_change = filtered['total_tax_change'].tolist() if 'total_tax_change' in filtered.columns else (filtered['mean_tax_change'] * filtered['property_count']).tolist()\n",
    "\n",
    "# Sort by median_pct_change ascending\n",
    "sorted_idx = np.argsort(median_pct_change)\n",
    "categories = [categories[i] for i in sorted_idx]\n",
    "counts = [counts[i] for i in sorted_idx]\n",
    "median_pct_change = [median_pct_change[i] for i in sorted_idx]\n",
    "median_dollar_change = [median_dollar_change[i] for i in sorted_idx]\n",
    "total_tax_change = [total_tax_change[i] for i in sorted_idx]\n",
    "\n",
    "# Custom color: anything above 0 is dark red, below 0 is green\n",
    "bar_colors = []\n",
    "for val in median_pct_change:\n",
    "    if val > 0:\n",
    "        bar_colors.append(\"#8B0000\")  # dark red\n",
    "    else:\n",
    "        bar_colors.append(\"#228B22\")  # professional green\n",
    "\n",
    "# Bar settings\n",
    "bar_height = 0.75\n",
    "fig_height = len(categories) * 0.8 + 1.2\n",
    "right_col_pad = 120  # more padding for right column\n",
    "fig, ax = plt.subplots(figsize=(17, fig_height))  # wider for right column\n",
    "\n",
    "y = np.arange(len(categories))\n",
    "\n",
    "# Draw bars\n",
    "ax.barh(\n",
    "    y, median_pct_change, color=bar_colors, edgecolor='none',\n",
    "    height=bar_height, alpha=0.92, linewidth=0, zorder=2\n",
    ")\n",
    "\n",
    "# Remove all spines and ticks for a clean look\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "\n",
    "# Adjusted vertical spacing\n",
    "cat_offset = 0.18   # less space between category and median\n",
    "med_offset = -0.03  # median just below category\n",
    "count_offset = -0.23  # more space below median for parcels\n",
    "\n",
    "# For right column: position for total tax change\n",
    "max_abs = max(abs(min(median_pct_change)), abs(max(median_pct_change)))\n",
    "right_col_x = max_abs + right_col_pad\n",
    "\n",
    "# Add Net Change header at the top of the right column\n",
    "ax.text(\n",
    "    right_col_x, len(categories) - 0.5, \"Net Change\", va='bottom', ha='left',\n",
    "    fontsize=15, fontweight='bold', color='black', fontname='Arial'\n",
    ")\n",
    "\n",
    "for i, (cat, val, count, med_dol, tot_change) in enumerate(zip(categories, median_pct_change, counts, median_dollar_change, total_tax_change)):\n",
    "    # Format median dollar and percent change together\n",
    "    if med_dol >= 0:\n",
    "        med_dol_str = f\"${med_dol:,.0f}\"\n",
    "    else:\n",
    "        med_dol_str = f\"-${abs(med_dol):,.0f}\"\n",
    "    pct_str = f\"{val:+.1f}%\"\n",
    "    median_combo = f\"Median: {med_dol_str}, {pct_str}\"\n",
    "\n",
    "    # Position: right of bar for positive, left for negative\n",
    "    if val < 0:\n",
    "        xpos = val - 2.5\n",
    "        ha = 'right'\n",
    "    else:\n",
    "        xpos = val + 2.5\n",
    "        ha = 'left'\n",
    "    # Category name (bold, bigger)\n",
    "    ax.text(\n",
    "        xpos, y[i]+cat_offset, cat, va='center', ha=ha,\n",
    "        fontsize=14, fontweight='bold', color='#222',\n",
    "        fontname='Arial'\n",
    "    )\n",
    "    # Median (dollar + percent, bold, black, just below category)\n",
    "    ax.text(\n",
    "        xpos, y[i]+med_offset, median_combo, va='center', ha=ha,\n",
    "        fontsize=12, fontweight='bold', color='black',\n",
    "        fontname='Arial'\n",
    "    )\n",
    "    # Count (bold, smaller, below median)\n",
    "    ax.text(\n",
    "        xpos, y[i]+count_offset, f\"{count:,} parcels\", va='center', ha=ha,\n",
    "        fontsize=11, fontweight='bold', color='#888',\n",
    "        fontname='Arial'\n",
    "    )\n",
    "    # Net change column, always right-aligned in a new column, black text, no \"Total:\"\n",
    "    if tot_change >= 0:\n",
    "        tot_change_str = f\"${tot_change:,.0f}\"\n",
    "    else:\n",
    "        tot_change_str = f\"-${abs(tot_change):,.0f}\"\n",
    "    ax.text(\n",
    "        right_col_x, y[i], tot_change_str, va='center', ha='left',\n",
    "        fontsize=13, fontweight='bold', color='black',\n",
    "        fontname='Arial'\n",
    "    )\n",
    "\n",
    "# Set x limits for symmetry, make bars longer, and leave space for right column\n",
    "ax.set_xlim(-right_col_x, right_col_x + 60)\n",
    "\n",
    "# Remove axis labels/ticks\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Use output_summary to generate categories and percent increase/decrease, filtering to count > 50\n",
    "\n",
    "# Filter to property_count > 50\n",
    "summary_filtered = output_summary[output_summary['property_count'] > 50].copy()\n",
    "\n",
    "# Sort by pct_increase_gt_threshold ascending (smallest percent increase first)\n",
    "summary_sorted = summary_filtered.sort_values('pct_increase_gt_threshold', ascending=True)\n",
    "\n",
    "categories_sorted = summary_sorted['PROPERTY_CATEGORY'].tolist()\n",
    "pct_increase_sorted = summary_sorted['pct_increase_gt_threshold'].tolist()\n",
    "pct_decrease_sorted = summary_sorted['pct_decrease_gt_threshold'].tolist()\n",
    "\n",
    "# Convert to integers for display\n",
    "pct_increase_int_sorted = [int(round(x)) for x in pct_increase_sorted]\n",
    "pct_decrease_int_sorted = [int(round(x)) for x in pct_decrease_sorted]\n",
    "\n",
    "y = np.arange(len(categories_sorted))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Use specified colors\n",
    "color_increase = \"#8B0000\"  # dark red\n",
    "color_decrease = \"#228B22\"  # professional green\n",
    "\n",
    "# Plot left (decrease) bars (green, to the left)\n",
    "ax.barh(\n",
    "    y, \n",
    "    [-v for v in pct_decrease_sorted], \n",
    "    color=color_decrease, \n",
    "    edgecolor='none', \n",
    "    height=0.7\n",
    ")\n",
    "\n",
    "# Plot right (increase) bars (red, to the right)\n",
    "ax.barh(\n",
    "    y, \n",
    "    pct_increase_sorted, \n",
    "    color=color_increase, \n",
    "    edgecolor='none', \n",
    "    height=0.7\n",
    ")\n",
    "\n",
    "# Add percent labels (integer, no decimals), smaller Arial font\n",
    "for i, (inc, dec) in enumerate(zip(pct_increase_int_sorted, pct_decrease_int_sorted)):\n",
    "    # Left side (decrease)\n",
    "    if dec > 0:\n",
    "        ax.text(\n",
    "            -dec - 2, y[i], f\"{dec}%\", \n",
    "            va='center', ha='right', \n",
    "            fontsize=8, fontweight='normal', color=color_decrease, fontname='Arial'\n",
    "        )\n",
    "    # Right side (increase)\n",
    "    if inc > 0:\n",
    "        ax.text(\n",
    "            inc + 2, y[i], f\"{inc}%\", \n",
    "            va='center', ha='left', \n",
    "            fontsize=8, fontweight='normal', color=color_increase, fontname='Arial'\n",
    "        )\n",
    "\n",
    "# Add category name at end of right bar, bold, smaller Arial, further from percent\n",
    "for i, (cat, inc) in enumerate(zip(categories_sorted, pct_increase_sorted)):\n",
    "    xpos = inc + 18 if inc > 0 else 18\n",
    "    ax.text(\n",
    "        xpos, y[i], cat, \n",
    "        va='center', ha='left', \n",
    "        fontsize=9, fontweight='bold', color='#222', fontname='Arial'\n",
    "    )\n",
    "\n",
    "# Remove all spines, ticks, and axis lines for minimalist look\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "\n",
    "# Remove grid, axis, and titles\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks([])\n",
    "ax.set_ylabel('')\n",
    "ax.set_xlabel('')\n",
    "ax.set_title('')\n",
    "\n",
    "# Set xlim for symmetry\n",
    "max_val = max(max(pct_increase_sorted), max(pct_decrease_sorted))\n",
    "ax.set_xlim(-max_val-20, max_val+48)\n",
    "\n",
    "# --- Add custom titles above left and right bars ---\n",
    "# Make the titles a little bit bigger and closer to the center\n",
    "title_fontsize = 10  # increased from 8\n",
    "title_color = 'black'\n",
    "title_fontweight = 'normal'\n",
    "title_fontname = 'Arial'\n",
    "\n",
    "# Compute center x for both titles, but offset slightly left/right of center\n",
    "title_y = len(categories_sorted) - 0.2\n",
    "\n",
    "# Left title (above left bars), closer to center\n",
    "left_title_x = -max_val * 0.45\n",
    "ax.text(\n",
    "    left_title_x, title_y, \n",
    "    \"Percent of parcels\\ndecreasing >10%\", \n",
    "    ha='center', va='bottom', fontsize=title_fontsize, fontweight=title_fontweight, \n",
    "    color=title_color, fontname=title_fontname, \n",
    "    bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.15')\n",
    ")\n",
    "\n",
    "# Right title (above right bars), closer to center\n",
    "right_title_x = max_val * 0.45\n",
    "ax.text(\n",
    "    right_title_x, title_y, \n",
    "    \"Percent of parcels\\nincreasing >10%\", \n",
    "    ha='center', va='bottom', fontsize=title_fontsize, fontweight=title_fontweight, \n",
    "    color=title_color, fontname=title_fontname, \n",
    "    bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.15')\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Understanding Property Types and Impacts\n",
    "\n",
    "With our split-rate tax calculated, we can now analyze which property types are most affected. Understanding the distribution of tax impacts across different property categories is crucial for policy makers and stakeholders.\n",
    "\n",
    "### Property Type Analysis\n",
    "\n",
    "We'll examine how the tax burden shifts across:\n",
    "- **Residential properties** (single-family, multi-family, condos)\n",
    "- **Commercial properties** (retail, office, industrial)  \n",
    "- **Vacant land** (often sees largest increases under LVT)\n",
    "- **Exempt properties** (government, religious, charitable)\n",
    "\n",
    "### Key Metrics to Track:\n",
    "- **Count**: Number of properties in each category\n",
    "- **Median tax change**: Typical impact (less affected by outliers)\n",
    "- **Average percentage change**: Overall magnitude of impact\n",
    "- **Percentage with increases**: How many properties see tax increases\n",
    "\n",
    "This analysis helps identify which sectors benefit from the LVT shift (typically developed properties) and which see increased burden (typically land-intensive properties with low improvement ratios).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_b.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Creating Detailed Property Categories\n",
    "\n",
    "To better understand impacts, we'll create a detailed property categorization system that groups similar property types together. This makes the analysis more meaningful and interpretable.\n",
    "\n",
    "The function below categorizes properties into groups like:\n",
    "- **Single Family** (with subcategories by lot size)\n",
    "- **Multi-Family** (small vs. large)\n",
    "- **Commercial** (by type: retail, office, industrial)\n",
    "- **Exempt** (by type: government, religious, charitable)\n",
    "\n",
    "This categorization helps us understand not just that \"residential\" properties are affected, but specifically which types of residential properties see the biggest changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Summary of Tax Impacts by Property Category\n",
    "\n",
    "Now we can see the clear patterns of how different property types are affected by the LVT shift. This table will show us:\n",
    "\n",
    "- **Which property types benefit** (negative changes = tax decreases)\n",
    "- **Which property types pay more** (positive changes = tax increases)  \n",
    "- **How concentrated the impacts are** (median vs. average differences)\n",
    "- **What percentage of each type sees increases**\n",
    "\n",
    "Generally, we expect:\n",
    "- **Developed properties** (houses, commercial buildings) to see tax **decreases**\n",
    "- **Vacant land** to see the **largest increases** \n",
    "- **Properties with high improvement-to-land ratios** to benefit most\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6: Adding Geographic Context\n",
    "\n",
    "To make our analysis spatially-aware, we need to add geographic boundaries to our parcel data. This enables us to:\n",
    "\n",
    "- **Create maps** showing tax changes across the city\n",
    "- **Analyze patterns by neighborhood** or district  \n",
    "- **Combine with demographic data** for equity analysis\n",
    "- **Present results visually** to stakeholders\n",
    "\n",
    "We'll fetch the parcel boundary data from the same ArcGIS service that contains the geometric information for each property.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Merging Tax Analysis with Geographic Data\n",
    "\n",
    "Here we combine our tax analysis results with the geographic boundaries. This creates a spatially-enabled dataset that allows us to:\n",
    "\n",
    "1. **Map tax changes** across South Bend\n",
    "2. **Identify spatial patterns** in tax impacts\n",
    "3. **Prepare for demographic analysis** by having geographic context\n",
    "\n",
    "The merge should give us the same number of records as our original analysis, now with geographic coordinates for each parcel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get census data for Spokane County (FIPS code: 53063)\n",
    "census_data, census_boundaries = get_census_data_with_boundaries(\n",
    "    fips_code='53063',  # Washington (53) + Spokane County (063)\n",
    "    year=2022\n",
    ")\n",
    "# Set CRS for census boundaries before merging\n",
    "census_boundaries = census_boundaries.set_crs(epsg=4326)  # Assuming WGS84 coordinate system\n",
    "boundary_gdf = df.set_crs(epsg=4326)  # Set same CRS for boundary data\n",
    "\n",
    "# Merge census data with our parcel boundaries\n",
    "df = match_to_census_blockgroups(\n",
    "    gdf=boundary_gdf,\n",
    "    census_gdf=census_boundaries,\n",
    "    join_type=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"Number of census blocks: {len(census_boundaries)}\")\n",
    "print(f\"Number of census data: {len(census_data)}\")\n",
    "print(f\"Number of parcels with census data: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 7: Demographic and Equity Analysis\n",
    "\n",
    "One of the most important aspects of LVT analysis is understanding the **equity implications** - how does the tax shift affect different income levels and demographic groups?\n",
    "\n",
    "### Adding Census Data\n",
    "\n",
    "We'll match each property to its Census Block Group and pull demographic data including:\n",
    "- **Median household income** \n",
    "- **Racial/ethnic composition**\n",
    "- **Population characteristics**\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Policy makers need to understand:\n",
    "- Does the LVT shift disproportionately burden low-income neighborhoods?\n",
    "- Are there racial equity implications?  \n",
    "- Does the policy align with broader equity goals?\n",
    "\n",
    "**Note**: You'll need a Census API key for this section. Get one free at: https://api.census.gov/data/key_signup.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DataFrame columns:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Exploring the Enhanced Dataset\n",
    "\n",
    "With census data merged in, our dataset now contains both property tax information and demographic context. Let's explore what variables we now have available for analysis.\n",
    "\n",
    "This enhanced dataset allows us to examine relationships between:\n",
    "- Property characteristics and demographics\n",
    "- Tax impacts and neighborhood income levels\n",
    "- Geographic patterns in tax burden shifts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all columns with maximum width\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Viewing the Complete Dataset\n",
    "\n",
    "Let's examine our enhanced dataset with all the variables we've created and merged. This gives us a comprehensive view of each property with:\n",
    "\n",
    "- **Property characteristics** (type, value, location)\n",
    "- **Current tax calculations** \n",
    "- **New LVT calculations**\n",
    "- **Tax change impacts**\n",
    "- **Demographic context** (income, race/ethnicity)\n",
    "\n",
    "This rich dataset forms the foundation for sophisticated equity and impact analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def filter_data(df):\n",
    "    \"\"\"Filter data to remove negative or zero median incomes and create non-vacant subset (also with positive income)\"\"\"\n",
    "    df_filtered = df[df['median_income'] > 0].copy()\n",
    "    non_vacant_df = df[(df['PROPERTY_CATEGORY'] != 'Vacant Land') & (df['median_income'] > 0)].copy()\n",
    "    return df_filtered, non_vacant_df\n",
    "\n",
    "def calculate_block_group_summary(df):\n",
    "    \"\"\"Calculate summary statistics for census block groups, excluding negative/zero median incomes\"\"\"\n",
    "    # Only include block groups with positive median income\n",
    "    df = df[df['median_income'] > 0].copy()\n",
    "    summary = df.groupby('std_geoid').agg(\n",
    "        median_income=('median_income', 'first'),\n",
    "        minority_pct=('minority_pct', 'first'),\n",
    "        black_pct=('black_pct', 'first'),\n",
    "        total_current_tax=('current_tax', 'sum'),\n",
    "        total_new_tax=('new_tax', 'sum'),\n",
    "        mean_tax_change=('tax_change', 'mean'),\n",
    "        median_tax_change=('tax_change', 'median'),\n",
    "        median_tax_change_pct=('tax_change_pct', 'median'),\n",
    "        parcel_count=('tax_change', 'count'),\n",
    "        has_vacant_land=('PROPERTY_CATEGORY', lambda x: 'Vacant Land' in x.values)\n",
    "    ).reset_index()\n",
    "    # Exclude block groups with non-positive median income (shouldn't be needed, but for safety)\n",
    "    summary = summary[summary['median_income'] > 0].copy()\n",
    "    summary['mean_tax_change_pct'] = ((summary['total_new_tax'] - summary['total_current_tax']) / \n",
    "                                    summary['total_current_tax']) * 100\n",
    "    return summary\n",
    "\n",
    "def create_scatter_plot(data, x_col, y_col, ax, title, xlabel, ylabel):\n",
    "    \"\"\"Create a scatter plot with trend line, excluding negative/zero incomes\"\"\"\n",
    "    # Exclude rows with non-positive x_col (e.g., median_income)\n",
    "    data = data[data[x_col] > 0].copy()\n",
    "    sns.scatterplot(\n",
    "        data=data,\n",
    "        x=x_col,\n",
    "        y=y_col,\n",
    "        size='parcel_count',\n",
    "        sizes=(20, 200),\n",
    "        alpha=0.7,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.axhline(y=0, color='r', linestyle='--')\n",
    "    \n",
    "    x = data[x_col].dropna()\n",
    "    y = data[y_col].dropna()\n",
    "    mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "    \n",
    "    if len(x[mask]) > 1:\n",
    "        z = np.polyfit(x[mask], y[mask], 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax.plot(x[mask], p(x[mask]), \"r--\")\n",
    "    \n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "\n",
    "def plot_comparison(data1, data2, x_col, y_col, title_prefix, xlabel):\n",
    "    \"\"\"Create side-by-side comparison plots, excluding negative/zero incomes\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "    create_scatter_plot(data1, x_col, y_col, ax1, \n",
    "                       f'{title_prefix} - All Properties', xlabel, 'Mean Tax Change (%)')\n",
    "    create_scatter_plot(data2, x_col, y_col, ax2,\n",
    "                       f'{title_prefix} - Excluding Vacant Land', xlabel, 'Mean Tax Change (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_correlations(data1, data2):\n",
    "    \"\"\"Calculate correlations between variables, excluding negative/zero incomes\"\"\"\n",
    "    correlations = {}\n",
    "    for df, suffix in [(data1, 'all'), (data2, 'non_vacant')]:\n",
    "        # Exclude rows with non-positive median_income for correlation\n",
    "        df_corr = df[df['median_income'] > 0].copy()\n",
    "        correlations[f'income_mean_{suffix}'] = df_corr[['median_income', 'mean_tax_change_pct']].corr().iloc[0, 1]\n",
    "        correlations[f'income_median_{suffix}'] = df_corr[['median_income', 'median_tax_change_pct']].corr().iloc[0, 1]\n",
    "        correlations[f'minority_mean_{suffix}'] = df_corr[['minority_pct', 'mean_tax_change_pct']].corr().iloc[0, 1]\n",
    "        correlations[f'black_mean_{suffix}'] = df_corr[['black_pct', 'mean_tax_change_pct']].corr().iloc[0, 1]\n",
    "    return correlations\n",
    "\n",
    "def weighted_median(values, weights):\n",
    "    \"\"\"Compute the weighted median of values with corresponding weights.\"\"\"\n",
    "    # Remove NaNs\n",
    "    mask = (~np.isnan(values)) & (~np.isnan(weights))\n",
    "    values = np.array(values)[mask]\n",
    "    weights = np.array(weights)[mask]\n",
    "    if len(values) == 0:\n",
    "        return np.nan\n",
    "    sorter = np.argsort(values)\n",
    "    values = values[sorter]\n",
    "    weights = weights[sorter]\n",
    "    cumsum = np.cumsum(weights)\n",
    "    cutoff = weights.sum() / 2.0\n",
    "    return values[np.searchsorted(cumsum, cutoff)]\n",
    "\n",
    "def create_quintile_summary(df, group_col, value_col):\n",
    "    \"\"\"Create summary statistics by quintiles, using mean/weighted-median tax change percent, excluding negative/zero incomes for income-based quintiles\"\"\"\n",
    "    # If grouping by income, exclude non-positive values\n",
    "    if group_col == 'median_income':\n",
    "        df = df[df['median_income'] > 0].copy()\n",
    "    df[f'{group_col}_quintile'] = pd.qcut(df[group_col], 5, \n",
    "                                         labels=[\"Q1 (Lowest)\", \"Q2\", \"Q3\", \"Q4\", \"Q5 (Highest)\"])\n",
    "    \n",
    "    def weighted_median_tax_change_pct(subdf):\n",
    "        # Use parcel_count as weights if available, else weight each row equally\n",
    "        if 'parcel_count' in subdf.columns:\n",
    "            weights = subdf['parcel_count']\n",
    "        else:\n",
    "            weights = np.ones(len(subdf))\n",
    "        return weighted_median(subdf['tax_change_pct'], weights)\n",
    "    \n",
    "    # For this context, each row is a parcel, so weight by 1 (or by parcel_count if already aggregated)\n",
    "    summary = df.groupby(f'{group_col}_quintile').apply(\n",
    "        lambda g: pd.Series({\n",
    "            'count': g['tax_change'].count(),\n",
    "            'mean_tax_change_pct': g['tax_change_pct'].mean(),\n",
    "            'median_tax_change_pct': weighted_median(g['tax_change_pct'], np.ones(len(g))),\n",
    "            'mean_value': g[value_col].mean()\n",
    "        })\n",
    "    ).reset_index()\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Main execution\n",
    "gdf_filtered, non_vacant_gdf = filter_data(df)\n",
    "print(f\"Number of rows in gdf_filtered: {len(gdf_filtered)}\")\n",
    "print(f\"Number of rows in non_vacant_gdf: {len(non_vacant_gdf)}\")\n",
    "\n",
    "# Calculate block group summaries (all with positive median_income only)\n",
    "census_block_groups = calculate_block_group_summary(gdf_filtered)\n",
    "non_vacant_block_summary = calculate_block_group_summary(non_vacant_gdf)\n",
    "\n",
    "# Create comparison plots (all with positive median_income only)\n",
    "plot_comparison(census_block_groups, non_vacant_block_summary, \n",
    "               'median_income', 'mean_tax_change_pct', \n",
    "               'Mean Tax Change vs. Median Income', \n",
    "               'Median Income by Census Block Group ($)')\n",
    "\n",
    "plot_comparison(census_block_groups, non_vacant_block_summary,\n",
    "               'minority_pct', 'mean_tax_change_pct',\n",
    "               'Mean Tax Change vs. Minority Percentage',\n",
    "               'Minority Population Percentage by Census Block Group')\n",
    "\n",
    "plot_comparison(census_block_groups, non_vacant_block_summary,\n",
    "               'black_pct', 'mean_tax_change_pct',\n",
    "               'Mean Tax Change vs. Black Percentage',\n",
    "               'Black Population Percentage by Census Block Group')\n",
    "\n",
    "# Calculate and print correlations (all with positive median_income only)\n",
    "correlations = calculate_correlations(census_block_groups, non_vacant_block_summary)\n",
    "for key, value in correlations.items():\n",
    "    print(f\"Correlation {key}: {value:.4f}\")\n",
    "\n",
    "# Create and display quintile summaries (income quintiles exclude negative/zero incomes)\n",
    "income_quintile_summary = create_quintile_summary(gdf_filtered, 'median_income', 'median_income')\n",
    "non_vacant_income_quintile_summary = create_quintile_summary(non_vacant_gdf, 'median_income', 'median_income')\n",
    "minority_quintile_summary = create_quintile_summary(gdf_filtered, 'minority_pct', 'minority_pct')\n",
    "non_vacant_minority_quintile_summary = create_quintile_summary(non_vacant_gdf, 'minority_pct', 'minority_pct')\n",
    "\n",
    "print(\"\\nTax impact by income quintile (all properties):\")\n",
    "display(income_quintile_summary)\n",
    "print(\"\\nTax impact by income quintile (excluding vacant land):\")\n",
    "display(non_vacant_income_quintile_summary)\n",
    "print(\"\\nTax impact by minority percentage quintile (all properties):\")\n",
    "display(minority_quintile_summary)\n",
    "print(\"\\nTax impact by minority percentage quintile (excluding vacant land):\")\n",
    "display(non_vacant_minority_quintile_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Median Income Quintiles vs. Mean Tax Change Percent (Census Block Groups)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    income_quintile_summary['median_income_quintile'],\n",
    "    income_quintile_summary['mean_tax_change_pct'],\n",
    "    marker='o',\n",
    "    label='All Properties'\n",
    ")\n",
    "plt.plot(\n",
    "    non_vacant_income_quintile_summary['median_income_quintile'],\n",
    "    non_vacant_income_quintile_summary['mean_tax_change_pct'],\n",
    "    marker='o',\n",
    "    label='Excluding Vacant Land'\n",
    ")\n",
    "plt.xlabel('Median Income Quintile')\n",
    "plt.ylabel('Mean Tax Change ($)')\n",
    "plt.title('Mean Tax Change by Median Income Quintile (Census Block Groups)')\n",
    "plt.legend()\n",
    "# Remove grid\n",
    "# Ensure x-axis at y=0 if negative values present\n",
    "ymin = min(\n",
    "    income_quintile_summary['mean_tax_change_pct'].min(),\n",
    "    non_vacant_income_quintile_summary['mean_tax_change_pct'].min()\n",
    ")\n",
    "ymax = max(\n",
    "    income_quintile_summary['mean_tax_change_pct'].max(),\n",
    "    non_vacant_income_quintile_summary['mean_tax_change_pct'].max()\n",
    ")\n",
    "if ymin < 0 < ymax:\n",
    "    plt.axhline(0, color='black', linewidth=1, linestyle='dotted')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Minority Percentage Quintiles vs. Mean Tax Change Percent (Census Block Groups)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    minority_quintile_summary['minority_pct_quintile'],\n",
    "    minority_quintile_summary['mean_tax_change_pct'],\n",
    "    marker='o',\n",
    "    label='All Properties'\n",
    ")\n",
    "plt.plot(\n",
    "    non_vacant_minority_quintile_summary['minority_pct_quintile'],\n",
    "    non_vacant_minority_quintile_summary['mean_tax_change_pct'],\n",
    "    marker='o',\n",
    "    label='Excluding Vacant Land'\n",
    ")\n",
    "plt.xlabel('Minority Percentage Quintile')\n",
    "plt.ylabel('Mean Tax Change ($)')\n",
    "plt.title('Mean Tax Change by Minority Percentage Quintile (Census Block Groups)')\n",
    "plt.legend()\n",
    "# Remove grid\n",
    "# Ensure x-axis at y=0 if negative values present\n",
    "ymin2 = min(\n",
    "    minority_quintile_summary['mean_tax_change_pct'].min(),\n",
    "    non_vacant_minority_quintile_summary['mean_tax_change_pct'].min()\n",
    ")\n",
    "ymax2 = max(\n",
    "    minority_quintile_summary['mean_tax_change_pct'].max(),\n",
    "    non_vacant_minority_quintile_summary['mean_tax_change_pct'].max()\n",
    ")\n",
    "if ymin2 < 0 < ymax2:\n",
    "    plt.axhline(0, color='black', linewidth=1, linestyle='dotted')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Median Tax Change by Neighborhood Median Income Excluding Vacant Land\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    non_vacant_income_quintile_summary['median_income_quintile'],\n",
    "    non_vacant_income_quintile_summary['median_tax_change_pct'],\n",
    "    marker='o',\n",
    "    label='Excluding Vacant Land'\n",
    ")\n",
    "plt.xlabel('Median Income Quintile')\n",
    "plt.ylabel('Median Tax Change ($)')\n",
    "plt.title('Median Tax Change by Neighborhood Median Income Excluding Vacant Land')\n",
    "ymin = non_vacant_income_quintile_summary['median_tax_change_pct'].min()\n",
    "ymax = non_vacant_income_quintile_summary['median_tax_change_pct'].max()\n",
    "# Ensure 0 is included on the y-axis\n",
    "plt.ylim(min(ymin, 0), max(ymax, 0) if ymax < 0 else max(ymax, 0, 1.05*ymax))\n",
    "plt.axhline(0, color='black', linewidth=1, linestyle='dotted')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot: Median Tax Change by Minority Percentage Quintile Excluding Vacant Land\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    non_vacant_minority_quintile_summary['minority_pct_quintile'],\n",
    "    non_vacant_minority_quintile_summary['median_tax_change_pct'],\n",
    "    marker='o',\n",
    "    label='Excluding Vacant Land'\n",
    ")\n",
    "plt.xlabel('Minority Percentage Quintile')\n",
    "plt.ylabel('Median Tax Change ($)')\n",
    "plt.title('Median Tax Change by Minority Percentage Quintile Excluding Vacant Land')\n",
    "ymin2 = non_vacant_minority_quintile_summary['median_tax_change_pct'].min()\n",
    "ymax2 = non_vacant_minority_quintile_summary['median_tax_change_pct'].max()\n",
    "# Ensure 0 is included on the y-axis\n",
    "plt.ylim(min(ymin2, 0), max(ymax2, 0) if ymax2 < 0 else max(ymax2, 0, 1.05*ymax2))\n",
    "plt.axhline(0, color='black', linewidth=1, linestyle='dotted')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set a modern style\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.15)\n",
    "\n",
    "# Upside Down Bar Graph: Median Tax Change by Neighborhood Median Income Excluding Vacant Land\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "vals = non_vacant_income_quintile_summary['median_tax_change_pct']\n",
    "labels = non_vacant_income_quintile_summary['median_income_quintile']\n",
    "\n",
    "# Color mapping: dark green (more negative) to light green (less negative)\n",
    "colors = sns.color_palette(\"Greens\", n_colors=len(vals))\n",
    "# Sort so that the most negative (largest magnitude) is darkest\n",
    "color_map = [colors[i] for i in np.argsort(np.argsort(-vals))]\n",
    "\n",
    "# To make bars start at the top and go down, invert the y-axis and plot positive heights\n",
    "bars = ax.bar(\n",
    "    labels,\n",
    "    np.abs(vals),\n",
    "    color=color_map,\n",
    "    edgecolor='black',\n",
    "    width=0.7\n",
    ")\n",
    "\n",
    "# Invert the y-axis so bars start at the top and go down\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Remove y-axis\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_title('Median Tax Change by Neighborhood Median Income (Excl. Vacant Land)', weight='bold', pad=30)\n",
    "\n",
    "# Remove all spines (including bottom)\n",
    "sns.despine(left=True, right=True, top=True, bottom=True)\n",
    "\n",
    "# Add value labels (bold, % sign) centered inside each bar (no line below the bar)\n",
    "for bar, val in zip(bars, vals):\n",
    "    ax.annotate(\n",
    "        f\"{val:.1f}%\",\n",
    "        xy=(bar.get_x() + bar.get_width() / 2, bar.get_height() / 2),\n",
    "        xytext=(0, 0),\n",
    "        textcoords=\"offset points\",\n",
    "        ha='center', va='center',\n",
    "        fontsize=13, color='black', fontweight='bold'\n",
    "    )\n",
    "\n",
    "# Move x-tick labels to the top\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.xticks(fontweight='bold')\n",
    "\n",
    "# Set y-limits to show bars going down from the top\n",
    "ymax = np.abs(vals).max() * 1.1\n",
    "ax.set_ylim(ymax, 0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Upside Down Bar Graph: Median Tax Change by Minority Percentage Quintile Excluding Vacant Land\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "vals2 = non_vacant_minority_quintile_summary['median_tax_change_pct']\n",
    "labels2 = non_vacant_minority_quintile_summary['minority_pct_quintile']\n",
    "\n",
    "colors2 = sns.color_palette(\"Greens\", n_colors=len(vals2))\n",
    "color_map2 = [colors2[i] for i in np.argsort(np.argsort(-vals2))]\n",
    "\n",
    "bars2 = ax.bar(\n",
    "    labels2,\n",
    "    np.abs(vals2),\n",
    "    color=color_map2,\n",
    "    edgecolor='black',\n",
    "    width=0.7\n",
    ")\n",
    "\n",
    "ax.invert_yaxis()\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_title('Median Tax Change by Minority Percentage Quintile (Excl. Vacant Land)', weight='bold', pad=30)\n",
    "sns.despine(left=True, right=True, top=True, bottom=True)\n",
    "\n",
    "for bar, val in zip(bars2, vals2):\n",
    "    ax.annotate(\n",
    "        f\"{val:.1f}%\",\n",
    "        xy=(bar.get_x() + bar.get_width() / 2, bar.get_height() / 2),\n",
    "        xytext=(0, 0),\n",
    "        textcoords=\"offset points\",\n",
    "        ha='center', va='center',\n",
    "        fontsize=13, color='black', fontweight='bold'\n",
    "    )\n",
    "\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.xticks(fontweight='bold')\n",
    "\n",
    "ymax2 = np.abs(vals2).max() * 1.1\n",
    "ax.set_ylim(ymax2, 0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict df to only residential property categories\n",
    "residential_categories = [\n",
    "    \"Single Family\",\n",
    "    \"Small Multi-Family (2-4 units)\"\n",
    "]\n",
    "df_residential = df[df['PROPERTY_CATEGORY'].isin(residential_categories)].copy()\n",
    "\n",
    "# --- Repeat the block group summary and quintile analysis for residential only ---\n",
    "\n",
    "# Filter data for residential (positive income, non-vacant)\n",
    "gdf_residential_filtered, non_vacant_residential_gdf = filter_data(df_residential)\n",
    "\n",
    "# Calculate block group summaries (all with positive median_income only, residential only)\n",
    "census_block_groups_res = calculate_block_group_summary(gdf_residential_filtered)\n",
    "non_vacant_block_summary_res = calculate_block_group_summary(non_vacant_residential_gdf)\n",
    "\n",
    "# Create comparison plots (all with positive median_income only, residential only)\n",
    "plot_comparison(\n",
    "    census_block_groups_res, non_vacant_block_summary_res, \n",
    "    'median_income', 'median_tax_change_pct', \n",
    "    'Median Tax Change vs. Median Income (Residential Only)', \n",
    "    'Median Income by Census Block Group ($)'\n",
    ")\n",
    "\n",
    "plot_comparison(\n",
    "    census_block_groups_res, non_vacant_block_summary_res,\n",
    "    'minority_pct', 'median_tax_change_pct',\n",
    "    'Median Tax Change vs. Minority Percentage (Residential Only)',\n",
    "    'Minority Population Percentage by Census Block Group'\n",
    ")\n",
    "\n",
    "plot_comparison(\n",
    "    census_block_groups_res, non_vacant_block_summary_res,\n",
    "    'black_pct', 'median_tax_change_pct',\n",
    "    'Median Tax Change vs. Black Percentage (Residential Only)',\n",
    "    'Black Population Percentage by Census Block Group'\n",
    ")\n",
    "\n",
    "# Calculate and print correlations (all with positive median_income only, residential only)\n",
    "correlations_res = calculate_correlations(census_block_groups_res, non_vacant_block_summary_res)\n",
    "for key, value in correlations_res.items():\n",
    "    print(f\"[Residential] Correlation {key}: {value:.4f}\")\n",
    "\n",
    "# Create and display quintile summaries (income quintiles exclude negative/zero incomes, residential only)\n",
    "income_quintile_summary_res = create_quintile_summary(gdf_residential_filtered, 'median_income', 'median_income')\n",
    "non_vacant_income_quintile_summary_res = create_quintile_summary(non_vacant_residential_gdf, 'median_income', 'median_income')\n",
    "minority_quintile_summary_res = create_quintile_summary(gdf_residential_filtered, 'minority_pct', 'minority_pct')\n",
    "non_vacant_minority_quintile_summary_res = create_quintile_summary(non_vacant_residential_gdf, 'minority_pct', 'minority_pct')\n",
    "\n",
    "print(\"\\n[Residential] Tax impact by income quintile (all properties):\")\n",
    "display(income_quintile_summary_res)\n",
    "print(\"\\n[Residential] Tax impact by income quintile (excluding vacant land):\")\n",
    "display(non_vacant_income_quintile_summary_res)\n",
    "print(\"\\n[Residential] Tax impact by minority percentage quintile (all properties):\")\n",
    "display(minority_quintile_summary_res)\n",
    "print(\"\\n[Residential] Tax impact by minority percentage quintile (excluding vacant land):\")\n",
    "display(non_vacant_minority_quintile_summary_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set a modern style\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.15)\n",
    "\n",
    "# Upside Down Bar Graph: Median Tax Change by Neighborhood Median Income Excluding Vacant Land (Residential Only)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "vals = non_vacant_income_quintile_summary_res['median_tax_change_pct']\n",
    "labels = non_vacant_income_quintile_summary_res['median_income_quintile']\n",
    "\n",
    "# Color mapping: dark green (more negative) to light green (less negative)\n",
    "colors = sns.color_palette(\"Greens\", n_colors=len(vals))\n",
    "# Sort so that the most negative (largest magnitude) is darkest\n",
    "color_map = [colors[i] for i in np.argsort(np.argsort(-vals))]\n",
    "\n",
    "# To make bars start at the top and go down, invert the y-axis and plot positive heights\n",
    "bars = ax.bar(\n",
    "    labels,\n",
    "    np.abs(vals),\n",
    "    color=color_map,\n",
    "    edgecolor='black',\n",
    "    width=0.7\n",
    ")\n",
    "\n",
    "# Invert the y-axis so bars start at the top and go down\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Remove y-axis\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_title('Median Tax Change by Neighborhood Median Income (Excl. Vacant Land, Residential Only)', weight='bold', pad=30)\n",
    "\n",
    "# Remove all spines (including bottom)\n",
    "sns.despine(left=True, right=True, top=True, bottom=True)\n",
    "\n",
    "# Add value labels (bold, % sign) centered inside each bar (no line below the bar)\n",
    "for bar, val in zip(bars, vals):\n",
    "    ax.annotate(\n",
    "        f\"{val:.1f}%\",\n",
    "        xy=(bar.get_x() + bar.get_width() / 2, bar.get_height() / 2),\n",
    "        xytext=(0, 0),\n",
    "        textcoords=\"offset points\",\n",
    "        ha='center', va='center',\n",
    "        fontsize=13, color='black', fontweight='bold'\n",
    "    )\n",
    "\n",
    "# Move x-tick labels to the top\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.xticks(fontweight='bold')\n",
    "\n",
    "# Set y-limits to show bars going down from the top\n",
    "ymax = np.abs(vals).max() * 1.1\n",
    "ax.set_ylim(ymax, 0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Upside Down Bar Graph: Median Tax Change by Minority Percentage Quintile Excluding Vacant Land (Residential Only)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "vals2 = non_vacant_minority_quintile_summary_res['median_tax_change_pct']\n",
    "labels2 = non_vacant_minority_quintile_summary_res['minority_pct_quintile']\n",
    "\n",
    "colors2 = sns.color_palette(\"Greens\", n_colors=len(vals2))\n",
    "color_map2 = [colors2[i] for i in np.argsort(np.argsort(-vals2))]\n",
    "\n",
    "bars2 = ax.bar(\n",
    "    labels2,\n",
    "    np.abs(vals2),\n",
    "    color=color_map2,\n",
    "    edgecolor='black',\n",
    "    width=0.7\n",
    ")\n",
    "\n",
    "ax.invert_yaxis()\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_title('Median Tax Change by Minority Percentage Quintile (Excl. Vacant Land, Residential Only)', weight='bold', pad=30)\n",
    "sns.despine(left=True, right=True, top=True, bottom=True)\n",
    "\n",
    "for bar, val in zip(bars2, vals2):\n",
    "    ax.annotate(\n",
    "        f\"{val:.1f}%\",\n",
    "        xy=(bar.get_x() + bar.get_width() / 2, bar.get_height() / 2),\n",
    "        xytext=(0, 0),\n",
    "        textcoords=\"offset points\",\n",
    "        ha='center', va='center',\n",
    "        fontsize=13, color='black', fontweight='bold'\n",
    "    )\n",
    "\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.xticks(fontweight='bold')\n",
    "\n",
    "ymax2 = np.abs(vals2).max() * 1.1\n",
    "ax.set_ylim(ymax2, 0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our new policy analysis functions\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "from policy_analysis import (\n",
    "    analyze_vacant_land, \n",
    "    analyze_parking_lots, \n",
    "    calculate_development_tax_penalty,\n",
    "    print_vacant_land_summary,\n",
    "    print_parking_analysis_summary, \n",
    "    print_development_penalty_summary\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Policy Analysis: Vacant Land Speculation\n",
    "\n",
    "# Let's see if we have owner information in our dataset\n",
    "print(\"Columns that might contain owner information:\")\n",
    "owner_cols = [col for col in df.columns if 'owner' in col.lower() or 'name' in col.lower()]\n",
    "print(owner_cols)\n",
    "\n",
    "# Filter out fully exempt properties for analysis\n",
    "df_non_exempt = df[df['full_exmp'] == 0].copy()\n",
    "print(f\"Excluding {(df['full_exmp'] == 1).sum():,} fully exempt properties from analysis\")\n",
    "print(f\"Analysis dataset: {len(df_non_exempt):,} properties (down from {len(df):,})\")\n",
    "\n",
    "# Run vacant land analysis excluding fully exempt land\n",
    "vacant_analysis = analyze_vacant_land(\n",
    "    df=df_non_exempt,\n",
    "    land_value_col='land_value',\n",
    "    property_type_col='prop_use_desc', \n",
    "    neighborhood_col='nbhd_name',\n",
    "    vacant_identifier='Vacant Land',\n",
    "    improvement_value_col='improvement_value',\n",
    "    exemption_col='exmp_amt',\n",
    "    exemption_flag_col='full_exmp'\n",
    ")\n",
    "\n",
    "# Print formatted summary\n",
    "print_vacant_land_summary(vacant_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Policy Analysis: Parking Lot Efficiency\n",
    "\n",
    "# Run parking lot analysis excluding fully exempt land\n",
    "parking_analysis = analyze_parking_lots(\n",
    "    df=df_non_exempt,\n",
    "    land_value_col='land_value',\n",
    "    improvement_value_col='improvement_value',\n",
    "    property_type_col='prop_use_desc',\n",
    "    parking_identifier='Trans - Parking',\n",
    "    min_land_value_threshold=50000,  # Focus on land worth $50k+\n",
    "    max_improvement_ratio=0.1,  # Improvement value <= 10% of land value\n",
    "    exemption_col='exmp_amt',\n",
    "    exemption_flag_col='full_exmp'\n",
    ")\n",
    "\n",
    "# Print formatted summary\n",
    "print_parking_analysis_summary(parking_analysis)\n",
    "\n",
    "# Show detailed breakdown by land value tier\n",
    "if 'by_land_value_tier' in parking_analysis:\n",
    "    print(\"\\nDetailed breakdown by land value tier:\")\n",
    "    print(parking_analysis['by_land_value_tier'].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import new property analysis functions\n",
    "from policy_analysis import (\n",
    "    analyze_property_values_by_category, \n",
    "    print_property_values_summary\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Property Value Analysis by Category\n",
    "\n",
    "# Analyze property values by category including exemptions\n",
    "property_values = analyze_property_values_by_category(\n",
    "    df=df,\n",
    "    category_col='PROPERTY_CATEGORY', \n",
    "    land_value_col='land_value',\n",
    "    improvement_value_col='improvement_value',\n",
    "    exemption_col='exmp_amt',\n",
    "    exemption_flag_col='full_exmp'\n",
    ")\n",
    "\n",
    "# Print formatted summary\n",
    "print_property_values_summary(property_values, \"Spokane Property Values by Category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Policy Analysis: Development Tax Penalty\n",
    "\n",
    "# Calculate the perverse incentive of building taxes\n",
    "# Using the current building millage rate from the split-rate system we modeled\n",
    "building_millage = 1.1238 / 1000  # Convert from per-thousand to decimal\n",
    "\n",
    "penalty_analysis = calculate_development_tax_penalty(\n",
    "    df=df,\n",
    "    improvement_value_col='improvement_value',\n",
    "    millage_rate=building_millage,  # Current building tax rate\n",
    "    years=30,  # 30-year analysis horizon\n",
    "    discount_rate=0.05,  # 5% discount rate\n",
    "    typical_construction_cost_per_sqft=200,  # Spokane construction costs\n",
    "    typical_unit_size_sqft=1200  # Typical housing unit size\n",
    ")\n",
    "\n",
    "# Print formatted summary\n",
    "print_development_penalty_summary(penalty_analysis)\n",
    "\n",
    "# Compare with LVT scenario (zero building tax)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: LVT ELIMINATES DEVELOPMENT PENALTY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lvt_penalty_analysis = calculate_development_tax_penalty(\n",
    "    df=df,\n",
    "    improvement_value_col='improvement_value',\n",
    "    millage_rate=0.0,  # No building tax under pure LVT\n",
    "    years=30,\n",
    "    discount_rate=0.05,\n",
    "    typical_construction_cost_per_sqft=200,\n",
    "    typical_unit_size_sqft=1200\n",
    ")\n",
    "\n",
    "print(f\"Current system development penalty: {penalty_analysis['equivalent_lost_units']:,.0f} housing units\")\n",
    "print(f\"LVT system development penalty: {lvt_penalty_analysis['equivalent_lost_units']:,.0f} housing units\")\n",
    "print(f\"Net housing units enabled by LVT: {penalty_analysis['equivalent_lost_units'] - lvt_penalty_analysis['equivalent_lost_units']:,.0f}\")\n",
    "\n",
    "# Show different scenarios with varying building tax rates\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCENARIO ANALYSIS: VARYING BUILDING TAX RATES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tax_rates = [0.005, 0.01, 0.015, 0.02, 0.025]  # 0.5% to 2.5%\n",
    "scenarios = []\n",
    "\n",
    "for rate in tax_rates:\n",
    "    scenario = calculate_development_tax_penalty(\n",
    "        df=df,\n",
    "        improvement_value_col='improvement_value',\n",
    "        millage_rate=rate,\n",
    "        years=30,\n",
    "        discount_rate=0.05,\n",
    "        typical_construction_cost_per_sqft=200,\n",
    "        typical_unit_size_sqft=1200\n",
    "    )\n",
    "    scenarios.append({\n",
    "        'Tax Rate (%)': rate * 100,\n",
    "        'NPV as % of Construction': scenario['npv_as_pct_of_construction_cost'],\n",
    "        'Equivalent Lost Units': scenario['equivalent_lost_units'],\n",
    "        'Lost Units as % of Stock': scenario['units_lost_percentage']\n",
    "    })\n",
    "\n",
    "scenario_df = pd.DataFrame(scenarios)\n",
    "print(scenario_df.round(1).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Land value by improvement share categories (non-exempt basis)\n",
    "from policy_analysis import analyze_land_by_improvement_share\n",
    "\n",
    "# Run for all parcels\n",
    "share_summary = analyze_land_by_improvement_share(\n",
    "    df=df,\n",
    "    land_value_col='land_value',\n",
    "    improvement_value_col='improvement_value',\n",
    "    exemption_col='exmp_amt',\n",
    "    exemption_flag_col='full_exmp'\n",
    ")\n",
    "\n",
    "print(\"Adjusted total land value (non-exempt): ${:,.0f}\".format(share_summary['total_adjusted_land_value']))\n",
    "for row in share_summary['categories']:\n",
    "    print(\"- {}: {:,} parcels | Adjusted land: ${:,.0f} ({:.1f}%)\".format(\n",
    "        row['category'], row['parcel_count'], row['adjusted_land_value'], row['share_of_total_land_value_pct']\n",
    "    ))\n",
    "\n",
    "# Now exclude vacant land and parking lots using PROPERTY_CATEGORY\n",
    "exclude_categories = [\"Vacant Land\", \"Transportation - Parking\"]\n",
    "df_non_vacant = df[~df[\"PROPERTY_CATEGORY\"].isin(exclude_categories)]\n",
    "\n",
    "share_summary_non_vacant = analyze_land_by_improvement_share(\n",
    "    df=df_non_vacant,\n",
    "    land_value_col='land_value',\n",
    "    improvement_value_col='improvement_value',\n",
    "    exemption_col='exmp_amt',\n",
    "    exemption_flag_col='full_exmp'\n",
    ")\n",
    "\n",
    "print(\"\\n(Excluding Vacant Land and Transportation - Parking)\")\n",
    "print(\"Adjusted total land value (non-exempt, non-vacant): ${:,.0f}\".format(share_summary_non_vacant['total_adjusted_land_value']))\n",
    "for row in share_summary_non_vacant['categories']:\n",
    "    print(\"- {}: {:,} parcels | Adjusted land: ${:,.0f} ({:.1f}%)\".format(\n",
    "        row['category'], row['parcel_count'], row['adjusted_land_value'], row['share_of_total_land_value_pct']\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate square footage for each parcel\n",
    "# If 'Shape__Area' is in square meters, convert to square feet (1 sq meter = 10.7639 sq ft)\n",
    "if 'Shape__Area' in df.columns:\n",
    "    df['sqft'] = df['Shape__Area'] * 10.7639\n",
    "else:\n",
    "    # If not available, try to calculate from geometry (assuming geometry is projected in meters)\n",
    "    df['sqft'] = df.geometry.area * 10.7639\n",
    "\n",
    "# Avoid division by zero\n",
    "df['sqft'] = df['sqft'].replace(0, pd.NA)\n",
    "\n",
    "# Calculate improvement value (assessed minus land, clipped at zero)\n",
    "df['improvement_value'] = (df['assessed_amt'] - df['land_value']).clip(lower=0)\n",
    "\n",
    "# Calculate per square foot columns\n",
    "df['land_value_per_sqft'] = df['land_value'] / df['sqft']\n",
    "df['assessed_amt_per_sqft'] = df['assessed_amt'] / df['sqft']\n",
    "df['improvement_value_per_sqft'] = df['improvement_value'] / df['sqft']\n",
    "df['tax_change_per_sqft'] = df['tax_change'] / df['sqft']\n",
    "df['new_tax_per_sqft'] = df['new_tax'] / df['sqft']\n",
    "df['current_tax_per_sqft'] = df['current_tax'] / df['sqft']\n",
    "\n",
    "# Ensure geometry is attached and select columns to save\n",
    "columns_to_save = [\n",
    "    'tax_change_pct', 'tax_change', 'new_tax', 'current_tax', 'PID_NUM', \n",
    "    'PROPERTY_CATEGORY', 'nbhd_code', 'prop_use_desc', 'site_address', \n",
    "    'assessed_amt', 'land_value', 'improvement_value',  'geometry',\n",
    "    'sqft', 'land_value_per_sqft', 'assessed_amt_per_sqft', 'improvement_value_per_sqft',\n",
    "    'tax_change_per_sqft', 'new_tax_per_sqft', 'current_tax_per_sqft'\n",
    "]\n",
    "\n",
    "# Save the DataFrame with geometry as a parquet file (restricted columns)\n",
    "df[columns_to_save].to_parquet('spokane_full_geom.parquet', index=False)\n",
    "\n",
    "# Also save the full DataFrame with geometry as a parquet file (all columns)\n",
    "df.to_parquet('spokane_full_geom_fullcols.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all geometries are Polygon or MultiPolygon and print the result\n",
    "is_all_polygons = df.geometry.geom_type.isin(['Polygon', 'MultiPolygon']).all()\n",
    "print(f\"All geometries are Polygon or MultiPolygon: {is_all_polygons}\")\n",
    "if not is_all_polygons:\n",
    "    # Try to convert to polygons if possible\n",
    "    df = df[df.geometry.type.isin(['Polygon', 'MultiPolygon'])].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define \"underdeveloped\" as: non-exempt and improvement_value / assessed_amt < 0.5\n",
    "# \"Vacant\" properties: PROPERTY_CATEGORY contains \"vacant\" (case-insensitive)\n",
    "# \"Non-exempt\": new_exemption is null/None/NaN or 0\n",
    "\n",
    "# 1. Vacant properties\n",
    "vacant_mask = df['PROPERTY_CATEGORY'].str.lower().str.contains('vacant', na=False)\n",
    "\n",
    "# 3. Underdeveloped: improvement_value / assessed_amt < 0.5\n",
    "underdeveloped_mask = (df['improvement_value'] / df['assessed_amt'] < 0.75)\n",
    "\n",
    "# Report the number of parcels that are vacant and number that are underdeveloped\n",
    "n_vacant = vacant_mask.sum()\n",
    "n_underdeveloped = underdeveloped_mask.sum()\n",
    "print(f\"Number of vacant parcels: {n_vacant}\")\n",
    "print(f\"Number of underdeveloped parcels: {n_underdeveloped}\")\n",
    "\n",
    "# Combine: vacant OR underdeveloped\n",
    "target_mask = vacant_mask | underdeveloped_mask\n",
    "\n",
    "# Calculate for target group\n",
    "target_df = df[target_mask]\n",
    "other_df = df[~target_mask]\n",
    "\n",
    "# Results for target group\n",
    "total_tax_change_target = target_df['tax_change'].sum()\n",
    "median_pct_target = target_df['tax_change_pct'].median()\n",
    "n_target = len(target_df)\n",
    "n_target_increase = (target_df['tax_change'] > 0).sum()\n",
    "n_target_decrease = (target_df['tax_change'] < 0).sum()\n",
    "pct_target_increase = 100 * n_target_increase / n_target if n_target > 0 else float('nan')\n",
    "pct_target_decrease = 100 * n_target_decrease / n_target if n_target > 0 else float('nan')\n",
    "\n",
    "# Results for all other parcels\n",
    "total_tax_change_other = other_df['tax_change'].sum()\n",
    "median_pct_other = other_df['tax_change_pct'].median()\n",
    "n_other = len(other_df)\n",
    "n_other_increase = (other_df['tax_change'] > 0).sum()\n",
    "n_other_decrease = (other_df['tax_change'] < 0).sum()\n",
    "pct_other_increase = 100 * n_other_increase / n_other if n_other > 0 else float('nan')\n",
    "pct_other_decrease = 100 * n_other_decrease / n_other if n_other > 0 else float('nan')\n",
    "\n",
    "print(\"Vacant or Underdeveloped Parcels:\")\n",
    "print(f\"  Total Tax Change ($): {total_tax_change_target:,.2f}\")\n",
    "print(f\"  Median Tax Change (%): {median_pct_target:.2f}\")\n",
    "print(f\"  Parcels with Tax Increase: {pct_target_increase:.1f}%\")\n",
    "print(f\"  Parcels with Tax Decrease: {pct_target_decrease:.1f}%\")\n",
    "\n",
    "print(\"\\nAll Other Parcels:\")\n",
    "print(f\"  Total Tax Change ($): {total_tax_change_other:,.2f}\")\n",
    "print(f\"  Median Tax Change (%): {median_pct_other:.2f}\")\n",
    "print(f\"  Parcels with Tax Increase: {pct_other_increase:.1f}%\")\n",
    "print(f\"  Parcels with Tax Decrease: {pct_other_decrease:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    display(df[df['PID_NUM'].isin(['35081.2213', '35081.2312'])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Export spokane.parquet with same columns as Syracuse ---\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Create export dataframe\n",
    "export_gdf = df.copy()\n",
    "\n",
    "# Create exemption flag as binary (1/0) for fully exempt properties\n",
    "export_gdf['exemption_flag'] = (export_gdf['full_exmp'] == True).astype(int)\n",
    "\n",
    "# Map property category (use existing PROPERTY_CATEGORY column)\n",
    "export_gdf['property_land_use_category'] = export_gdf['PROPERTY_CATEGORY']\n",
    "\n",
    "# Create refined property/land use category with three options: Vacant, Parking Lot, Underdeveloped\n",
    "def categorize_property_refined(row):\n",
    "    \"\"\"Categorize properties into refined categories\"\"\"\n",
    "    category = row['PROPERTY_CATEGORY']\n",
    "    if 'Vacant' in str(category):\n",
    "        return 'Vacant'\n",
    "    elif 'Parking' in str(category):\n",
    "        return 'Parking Lot'\n",
    "    elif row['improvement_value'] < 0.5 * (row['land_value'] + row['improvement_value']):\n",
    "        return 'Underdeveloped'\n",
    "    else:\n",
    "        return None  # null for all other categories\n",
    "\n",
    "export_gdf['property_land_use_refined'] = export_gdf.apply(categorize_property_refined, axis=1)\n",
    "\n",
    "# Calculate area in square feet from geometry (Shape__Area is already in square feet or convert from sq meters)\n",
    "if 'Shape__Area' in export_gdf.columns:\n",
    "    # Assume Shape__Area is in square meters, convert to square feet\n",
    "    export_gdf['area_sqft'] = export_gdf['Shape__Area'] * 10.7639\n",
    "else:\n",
    "    # Fallback: calculate from geometry\n",
    "    export_gdf['area_sqft'] = export_gdf.geometry.area * 10.7639\n",
    "\n",
    "# Calculate current tax per square foot\n",
    "export_gdf['current_tax_per_sqft'] = np.where(\n",
    "    export_gdf['area_sqft'] > 0,\n",
    "    export_gdf['current_tax'] / export_gdf['area_sqft'],\n",
    "    0\n",
    ")\n",
    "\n",
    "# Calculate land value per square foot\n",
    "export_gdf['land_value_per_sqft'] = np.where(\n",
    "    export_gdf['area_sqft'] > 0,\n",
    "    export_gdf['land_value'] / export_gdf['area_sqft'],\n",
    "    0\n",
    ")\n",
    "\n",
    "# Calculate improvement value per square foot\n",
    "export_gdf['improvement_value_per_sqft'] = np.where(\n",
    "    export_gdf['area_sqft'] > 0,\n",
    "    export_gdf['improvement_value'] / export_gdf['area_sqft'],\n",
    "    0\n",
    ")\n",
    "\n",
    "# Select columns for export, matching Syracuse structure\n",
    "columns_to_export = [\n",
    "    'geometry',\n",
    "    'exemption_flag',\n",
    "    'property_land_use_category',\n",
    "    'property_land_use_refined',\n",
    "    'current_tax',\n",
    "    'current_tax_per_sqft',\n",
    "    'land_value',\n",
    "    'land_value_per_sqft',\n",
    "    'improvement_value',\n",
    "    'improvement_value_per_sqft'\n",
    "]\n",
    "\n",
    "# Rename columns to match Syracuse naming convention\n",
    "export_final = export_gdf[columns_to_export].rename(columns={\n",
    "    'land_value': 'current_full_land_value'\n",
    "})\n",
    "\n",
    "# Ensure geometry is valid\n",
    "export_final['geometry'] = export_final['geometry'].apply(lambda geom: geom if geom is None or geom.is_valid else geom.buffer(0))\n",
    "\n",
    "# Ensure output is in WGS84 (EPSG:4326) before saving\n",
    "if export_final.crs is None or export_final.crs.to_epsg() != 4326:\n",
    "    export_final = export_final.to_crs(\"EPSG:4326\")\n",
    "    print(\"Converted to EPSG:4326\")\n",
    "\n",
    "# Save as Parquet\n",
    "output_filename = os.path.expanduser(\"~/Downloads/spokane.parquet\")\n",
    "export_final.to_parquet(output_filename, index=False)\n",
    "\n",
    "print(f\"\\n✅ Saved spokane.parquet to Downloads\")\n",
    "print(\"Saved columns:\", export_final.columns.tolist())\n",
    "print(\"Property refined category counts:\")\n",
    "print(export_final['property_land_use_refined'].value_counts(dropna=False))\n",
    "print(\"Property category counts:\")\n",
    "print(export_final['property_land_use_category'].value_counts().head(10))\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\n👀 First 5 rows of exported data:\")\n",
    "display(export_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def load_owner_info(pid):\n",
    "    \"\"\"\n",
    "    Loads the owner information for a given PID from the Spokane County property information site.\n",
    "    Returns a dictionary with keys: owner_name, owner_address.\n",
    "    \"\"\"\n",
    "    url = f\"https://cp.spokanecounty.org/SCOUT/propertyinformation/Summary.aspx?PID={pid}\"\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    table = soup.find(\"table\", id=\"MainContent_OwnerName_dlOwner\")\n",
    "    if table is None:\n",
    "        raise ValueError(\"Could not find owner info table in the page.\")\n",
    "\n",
    "    # Find the owner name and address spans\n",
    "    owner_name_span = table.find(\"span\", id=\"MainContent_OwnerName_dlOwner_txtNameLabel_0\")\n",
    "    owner_address_span = table.find(\"span\", id=\"MainContent_OwnerName_dlOwner_addressLabel_0\")\n",
    "\n",
    "    owner_name = owner_name_span.get_text(strip=True) if owner_name_span else None\n",
    "    owner_address = owner_address_span.get_text(strip=True) if owner_address_span else None\n",
    "\n",
    "    return {\n",
    "        \"owner_name\": owner_name,\n",
    "        \"owner_address\": owner_address\n",
    "    }\n",
    "\n",
    "# Test the function with the given PID\n",
    "test_pid = \"35182.4601\"\n",
    "owner_info = load_owner_info(test_pid)\n",
    "print(owner_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scrape_data == 1:\n",
    "    import pandas as pd\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    from tqdm import tqdm\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    # Assume df is already loaded in the notebook and contains a 'PID_NUM' column\n",
    "    # Remove any rows with missing or null PID_NUM\n",
    "    pids = df['PID_NUM'].dropna().unique().tolist()\n",
    "\n",
    "    results = []\n",
    "    output_path = os.path.join(\"data\", \"spokane\", \"owner_info.csv\")\n",
    "\n",
    "    def safe_load_owner_info(pid):\n",
    "        try:\n",
    "            info = load_owner_info(pid)\n",
    "            return {\"pid\": pid, \"owner_name\": info[\"owner_name\"], \"owner_address\": info[\"owner_address\"]}\n",
    "        except Exception as e:\n",
    "            return {\"pid\": pid, \"owner_name\": None, \"owner_address\": None, \"error\": str(e)}\n",
    "\n",
    "    max_workers = min(32, (os.cpu_count() or 1) * 5)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_pid = {executor.submit(safe_load_owner_info, pid): pid for pid in pids}\n",
    "        for future in tqdm(as_completed(future_to_pid), total=len(future_to_pid), desc=\"Fetching owner info\"):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "            # Optional: sleep a tiny bit to be nice to the server\n",
    "            time.sleep(0.01)\n",
    "\n",
    "    owner_info_df = pd.DataFrame(results)\n",
    "    owner_info_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved owner info for {len(owner_info_df)} parcels to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have run the owner info scraping above, load the resulting CSV and join it to your main dataframe.\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "owner_info_path = os.path.join(\"data\", \"spokane\", \"owner_info.csv\")\n",
    "if os.path.exists(owner_info_path):\n",
    "    owner_info_df = pd.read_csv(owner_info_path)\n",
    "    # If 'df' is your main dataframe, join on PID_NUM (or 'pid' if that's the column in owner_info_df)\n",
    "    # Make sure both columns are string type for a clean join\n",
    "    df['PID_NUM'] = df['PID_NUM'].astype(str)\n",
    "    owner_info_df['pid'] = owner_info_df['pid'].astype(str)\n",
    "    df = df.merge(owner_info_df, left_on='PID_NUM', right_on='pid', how='left')\n",
    "    print(\"Owner info joined. Example rows:\")\n",
    "    display(df[['PID_NUM', 'owner_name', 'owner_address']].head())\n",
    "else:\n",
    "    print(f\"Owner info file not found at {owner_info_path}. Run the scraping cell above first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_missing_owner_name = owner_info_df['owner_name'].isna().sum()\n",
    "num_missing_owner_address = owner_info_df['owner_address'].isna().sum()\n",
    "print(f\"Number of missing owner_name: {num_missing_owner_name}\")\n",
    "print(f\"Number of missing owner_address: {num_missing_owner_address}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For most addresses (3 commas), split into: street, city, state, zip\n",
    "def split_address(row):\n",
    "    addr = str(row['owner_address'])\n",
    "    num_commas = addr.count(',')\n",
    "    if num_commas == 3:\n",
    "        # Standard: street, city, state, zip\n",
    "        parts = [p.strip() for p in addr.split(',')]\n",
    "        if len(parts) == 4:\n",
    "            return pd.Series({\n",
    "                'street': parts[0],\n",
    "                'city': parts[1],\n",
    "                'state': parts[2],\n",
    "                'zip': parts[3]\n",
    "            })\n",
    "        else:\n",
    "            return pd.Series({'street': None, 'city': None, 'state': None, 'zip': None})\n",
    "    elif num_commas == 4:\n",
    "        # Extra comma in street address: ignore the first comma, then split the rest\n",
    "        first_comma = addr.find(',')\n",
    "        rest = addr[first_comma+1:]\n",
    "        # Now split rest into 3 parts\n",
    "        parts = [addr[:first_comma].strip()] + [p.strip() for p in rest.split(',', 3)]\n",
    "        if len(parts) == 4:\n",
    "            return pd.Series({\n",
    "                'street': parts[0] + ', ' + parts[1],  # combine street parts\n",
    "                'city': parts[2],\n",
    "                'state': parts[3],\n",
    "                'zip': parts[4] if len(parts) > 4 else None\n",
    "            })\n",
    "        else:\n",
    "            # Defensive fallback: try to split into 5 and combine first two as street\n",
    "            parts = [p.strip() for p in addr.split(',', 4)]\n",
    "            if len(parts) == 5:\n",
    "                return pd.Series({\n",
    "                    'street': parts[0] + ', ' + parts[1],\n",
    "                    'city': parts[2],\n",
    "                    'state': parts[3],\n",
    "                    'zip': parts[4]\n",
    "                })\n",
    "            else:\n",
    "                return pd.Series({'street': None, 'city': None, 'state': None, 'zip': None})\n",
    "    else:\n",
    "        # For 0 commas or other cases, return None\n",
    "        return pd.Series({'street': None, 'city': None, 'state': None, 'zip': None})\n",
    "\n",
    "# Apply the function to all rows\n",
    "address_split = owner_info_df.apply(split_address, axis=1)\n",
    "owner_info_df = pd.concat([owner_info_df, address_split], axis=1)\n",
    "\n",
    "# Print the count of states\n",
    "print(\"State value counts:\")\n",
    "print(owner_info_df['state'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Chart: Calculate actual percentage values for the income quintile chart\n",
    "\n",
    "# First, let's calculate the actual median percentage change for each quintile\n",
    "def calculate_median_percentage_by_quintile(df, group_col, value_col):\n",
    "    \"\"\"Calculate median percentage change by quintiles\"\"\"\n",
    "    # If grouping by income, exclude non-positive values\n",
    "    if group_col == 'median_income':\n",
    "        df = df[df['median_income'] > 0].copy()\n",
    "    \n",
    "    df[f'{group_col}_quintile'] = pd.qcut(df[group_col], 5, \n",
    "                                         labels=[\"Q1 (Lowest)\", \"Q2\", \"Q3\", \"Q4\", \"Q5 (Highest)\"])\n",
    "    \n",
    "    summary = df.groupby(f'{group_col}_quintile').agg(\n",
    "        count=('tax_change', 'count'),\n",
    "        mean_tax_change=('tax_change', 'mean'),\n",
    "        median_tax_change=('tax_change', 'median'),\n",
    "        median_tax_change_pct=('tax_change_pct', 'median'),  # This is the key addition\n",
    "        mean_value=(value_col, 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Recalculate the income quintile summary with percentage data\n",
    "non_vacant_income_quintile_summary_pct = calculate_median_percentage_by_quintile(\n",
    "    non_vacant_gdf, 'median_income', 'median_income'\n",
    ")\n",
    "\n",
    "print(\"Corrected Income Quintile Summary (with percentages):\")\n",
    "display(non_vacant_income_quintile_summary_pct)\n",
    "\n",
    "# CORRECTED Chart: Median Tax Change PERCENTAGE by Income Quintile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set a modern style\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.15)\n",
    "\n",
    "# Corrected Bar Graph: Median Tax Change PERCENTAGE by Neighborhood Median Income Excluding Vacant Land\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Use the ACTUAL PERCENTAGE values, not dollar amounts with % signs\n",
    "vals = non_vacant_income_quintile_summary_pct['median_tax_change_pct']\n",
    "labels = non_vacant_income_quintile_summary_pct['median_income_quintile']\n",
    "\n",
    "# Color mapping: dark green (more negative) to light green (less negative)\n",
    "colors = sns.color_palette(\"Greens\", n_colors=len(vals))\n",
    "# Sort so that the most negative (largest magnitude) is darkest\n",
    "color_map = [colors[i] for i in np.argsort(np.argsort(-vals))]\n",
    "\n",
    "# To make bars start at the top and go down, invert the y-axis and plot positive heights\n",
    "bars = ax.bar(\n",
    "    labels,\n",
    "    np.abs(vals),\n",
    "    color=color_map,\n",
    "    edgecolor='black',\n",
    "    width=0.7\n",
    ")\n",
    "\n",
    "# Invert the y-axis so bars start at the top and go down\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Remove y-axis\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_title('CORRECTED: Median Tax Change PERCENTAGE by Income Quintile (Excl. Vacant Land)', \n",
    "             weight='bold', pad=30)\n",
    "\n",
    "# Remove all spines (including bottom)\n",
    "sns.despine(left=True, right=True, top=True, bottom=True)\n",
    "\n",
    "# Add value labels (bold, % sign) centered inside each bar - NOW WITH ACTUAL PERCENTAGES\n",
    "for bar, val in zip(bars, vals):\n",
    "    ax.annotate(\n",
    "        f\"{val:.1f}%\",\n",
    "        xy=(bar.get_x() + bar.get_width() / 2, bar.get_height() / 2),\n",
    "        xytext=(0, 0),\n",
    "        textcoords=\"offset points\",\n",
    "        ha='center', va='center',\n",
    "        fontsize=13, color='black', fontweight='bold'\n",
    "    )\n",
    "\n",
    "# Move x-tick labels to the top\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.xticks(fontweight='bold')\n",
    "\n",
    "# Set y-limits to show bars going down from the top\n",
    "ymax = np.abs(vals).max() * 1.1\n",
    "ax.set_ylim(ymax, 0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\nExplanation of the fix:\")\n",
    "print(\"- Previous chart showed DOLLAR amounts ($) with percentage signs (%) - mathematically impossible\")\n",
    "print(\"- This corrected chart shows actual PERCENTAGE changes\")\n",
    "print(\"- Values like -109% would mean impossible negative taxes; real percentages are much smaller\")\n",
    "print(\"- The tax change percentages are calculated as: (new_tax - current_tax) / current_tax * 100\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the unique cities\n",
    "print(\"\\nUnique cities:\")\n",
    "print(owner_info_df['city'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(owner_info_df.columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
